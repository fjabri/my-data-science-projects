{"cells": [{"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "CDEWTP-vNB_R"}, "source": "---\n# Pr\u00e9paration des Donn\u00e9es\n\n---\n\n\nDans cette s\u00e9quence, nous allons voir diff\u00e9rentes techniques permettant de pr\u00e9parer ad\u00e9quatement les donn\u00e9es avant de les fournir \u00e0 un ou plusieurs algorithmes d'apprentissage automatique.\n\n<center><img src=\"https://python.gel.ulaval.ca/media/sio-u009/mlprocess_2.png\" alt=\"Processus d'apprentissage automatique\" width=\"50%\"/></center>\n\nNous aborderons notamment les points suivants :\n\n1. **Le nettoyage et les aberrations statistiques**\n2. L'imputation de donn\u00e9es manquantes\n3. \u00c9quilibrage de donn\u00e9es d\u00e9s\u00e9quilibr\u00e9es\n4. Transformation des caract\u00e9ristiques\n    1. *rescaling* et *normalizing* (\\[0, 1\\] ou \\[-1, 1\\]), *standardizing* (loi normale)\n    2. Repr\u00e9sentation matricielle de donn\u00e9es cat\u00e9goris\u00e9es\n    3. R\u00e9duction de la dimensionnalit\u00e9 ou cr\u00e9ation de caract\u00e9ristiques\n\nEt nous commencons dans ce premier module par les *outliers* ou aberrations statistiques.\n\n"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "JqvX_mrjJYFd"}, "source": "## 1. Nettoyage et Aberrations\n### Identifier les donn\u00e9es dupliqu\u00e9es\n\n**Compter les valeurs uniques d'une colonne:**\n\n```\ndf.groupby('ma_categorie').id_user.nunique()\n```\n\n**Montrer les valeurs dupliqu\u00e9es dans une colonne:**\n\n```\npd.concat(i for _, i in df.groupby('id_user') if len(i) > 1)\n```\n\nDans le cas o\u00f9 vous voudriez montrer les valeurs dupliqu\u00e9es pour une combinaison de colonnes, remplacez `id_user` par la liste des colonnes \u00e0 consid\u00e9rer (e.g.  ['id_user', 'pays'])\n\n### Retirer les donn\u00e9es dupliqu\u00e9es\n\n```\ndf = df.drop_duplicates(subset='id_user', keep=False)\n```  \n\n### Identifier les donn\u00e9es aberrantes\n\nCette s\u00e9quence est  inspir\u00e9e des exemples de code de la librairie `scikit-learn` de [Alexandre Gramfort](mailto:alexandre.gramfort@inria.fr) et [Albert Thomas](mailto:albert.thomas@telecom-paristech.fr) (License: BSD 3 clause)\n\nNous allons commencer par g\u00e9n\u00e9rer plusieurs ensembles de donn\u00e9es en y incluant des donn\u00e9es abberantes (ne faisant pas partie de la distribution r\u00e9elle des donn\u00e9es \u00e0 mod\u00e9liser), puis nous allons vous montrer diverses techniques permettant (avec plus ou moins de succ\u00e8s) de d\u00e9terminer quelles donn\u00e9es font partie de la distribution initiale, et lesquelles n'en font pas partie.\n\n## N'h\u00e9sitez pas \u00e0 jouer avec les param\u00e8tres des diff\u00e9rents algorithmes pour voir l'implication de chacun."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# Import des librairies n\u00e9cessaires\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.rcParams['contour.negative_linestyle'] = 'solid'\nfrom sklearn.datasets import make_moons, make_blobs"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# Param\u00e8tres des exemples\nn_samples = 300\n# Quantit\u00e9 de donn\u00e9es aberrantes (en %)\noutliers_fraction = 0.25\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "NZ8oA64iJYFd", "trusted": true}, "outputs": [], "source": "# Definition des datasets avec donn\u00e9es aberrantes selon les param\u00e8tres pr\u00e9c\u00e9dents\nn_outliers = int(outliers_fraction * n_samples)\nn_inliers = n_samples - n_outliers\n\nblobs_params = dict(random_state=0, n_samples=n_inliers, n_features=2)\ndatasets = [# 5 datasets:\n    # Un seul blob\n    make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5,\n               **blobs_params)[0],\n    # deux blobs (distribution chameau)\n    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5],\n               **blobs_params)[0],\n    # deux blobs dont un plus \u00e9tal\u00e9\n    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, .3],\n               **blobs_params)[0],\n    # Deux lunes\n    4. * (make_moons(n_samples=n_samples, noise=.05, random_state=0)[0] -\n          np.array([0.5, 0.25])),\n    # Bruit blanc\n    14. * (np.random.RandomState(42).rand(n_samples, 2) - 0.5)]\n\n# Mesh pour afficher le contour de d\u00e9cision\nxx, yy = np.meshgrid(np.linspace(-7, 7, 150),\n                     np.linspace(-7, 7, 150))"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# fonction d'affichage des datasets avec coloration de l'algorithme et ajout d'outliers\ndef plot_datasets(name='', algorithm=None, addOutliers=True):\n    # Taille des figures\n    plt.figure(figsize=(len(datasets) * 2 + 3, 3))\n    plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n                        hspace=.01)\n    \n    plot_num = 1\n    # Garantit la reproductibilit\u00e9\n    rng = np.random.RandomState(42)\n    \n    for i_dataset, X in enumerate(datasets):\n        \n        y_pred = np.repeat(0,len(X))\n        # Add outliers\n        if addOutliers:\n            X = np.concatenate([X, rng.uniform(low=-6, high=6,size=(n_outliers, 2))], axis=0)\n            #On met la v\u00e9rit\u00e9 dans la pr\u00e9diction si on ne fait pas de pr\u00e9diction\n            y_pred = np.concatenate([y_pred,np.repeat(1,n_outliers)],axis=0)\n\n        t0 = 0\n        t1 = 0\n        ax = plt.subplot(1,len(datasets), plot_num)\n        # Si on veut voir la pr\u00e9diction dans l'affichage : \n        if algorithm != None:\n            t0 = time.time()\n            algorithm.fit(X)\n            t1 = time.time()\n            if i_dataset == 0:\n                plt.title(name, size=18)\n            # Fit les donn\u00e9es et tag les outliers\n            if name == \"Local Outlier Factor\": # LOF n'impl\u00e9mente pas predict\n                y_pred = algorithm.fit_predict(X)\n            elif name == \"KDE\": # KDE non plus\n                # Score samples\n                pred = np.exp(algorithm.fit(X).score_samples(X))\n                n = sum(pred < 0.05)\n                outlier_ind = np.asarray(pred).argsort()[:n]\n                y_pred = np.array([-1 if i in outlier_ind else 1 for i in range(len(X))])\n            elif name == \"Tukey\": # Et celui la non plus ... obviously\n                outlier_ind = algorithm.outliers(X)\n                y_pred = np.array([-1 if i in outlier_ind else 1 for i in range(len(X))])\n            else: \n                y_pred = algorithm.fit(X).predict(X)\n                # Affichage des points et de la ligne de d\u00e9marquation\n                Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])\n                Z = Z.reshape(xx.shape)\n                plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\n            plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'), transform=plt.gca().transAxes, size=15, horizontalalignment='right')\n\n        colors = np.array(['#377eb8', '#ff7f00'])\n        ax.scatter(X[:, 0], X[:, 1], s=10, color=colors[(y_pred + 1) // 2])\n        plt.xlim(-7, 7)\n        plt.ylim(-7, 7)\n        plt.xticks(())\n        plt.yticks(())\n        plot_num += 1\n    plt.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# Affichage des donn\u00e9es sans outliers\nplot_datasets(addOutliers=False)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "V1khl9iVJYFf", "trusted": true}, "outputs": [], "source": "#Affichage avec outliers\nplot_datasets(addOutliers=True)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "\u00c0 partir d'ici nous allons essayer diff\u00e9rents algorithmes de d\u00e9tection des *outliers*, les donn\u00e9es hors distribution. Pour chacune des fonctions utilis\u00e9en 'h\u00e9sitez pas \u00e0 vous r\u00e9f\u00e9rer \u00e0 la documentation en ligne disponible ici : [SciKit-Learn documentation](https://scikit-learn.org/stable/).\n\nG\u00e9n\u00e9ralement, une simple recherche sur Google de la fonction (par exemple ci-dessous, chercher *EllipticEnvelope* sur [Google](https://www.google.com/search?q=EllipticEnvelope) vous m\u00e8nera directement [\u00e0 la bonne page de documentation].)\n\nLa partie importante de cette documentation qui n'est pas repr\u00e9sent\u00e9e dans ce *notebook* est l'ensemble des param\u00e8tres arbitrairement fix\u00e9s \u00e0 leurs valeurs par d\u00e9faut que vous voudriez peut-\u00eatre essayer de corriger vu que vous connaissez d\u00e9j\u00e0 quelles sont les \"bonnes\" donn\u00e9es et les \"mauvaises\" donn\u00e9es."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "aFjsNiW9JYFg"}, "source": "#### Covariance robuste"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "kfeqHZgAJYFg", "trusted": true}, "outputs": [], "source": "from sklearn.covariance import EllipticEnvelope\n\nalgo =  (\"Robust covariance\", EllipticEnvelope(contamination=outliers_fraction))\n\nplot_datasets(*algo)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "TY90K5fiJYFh"}, "source": "#### SVM mono-classe"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "oMNB0zHyJYFh", "trusted": true}, "outputs": [], "source": "from sklearn import svm\n\nalgo = (\"One-Class SVM\", svm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\", gamma=0.3))\n\nplot_datasets(*algo)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "ggIIlyNwJYFi"}, "source": "#### For\u00eat d'Isolation"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "V_gYyVbnJYFi", "trusted": true}, "outputs": [], "source": "from sklearn.ensemble import IsolationForest\n\nalgo = (\"Isolation Forest\", IsolationForest(contamination=outliers_fraction, random_state=42))\n\nplot_datasets(*algo)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "lhMt8YdKJYFj"}, "source": "#### Facteur d'aberration local"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "PUny__cUJYFj", "trusted": true}, "outputs": [], "source": "from sklearn.neighbors import LocalOutlierFactor\n\nalgo = (\"Local Outlier Factor\", LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction))\n\nplot_datasets(*algo)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "PejhlHCqJYFl"}, "source": "#### Test de Tukey pour les valeurs extr\u00eames"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "fhYk1yezJYFl", "trusted": true}, "outputs": [], "source": "ecart = 1.5\n\n# Defini la fonction qui utilise les deviations interquartile avec les quartiles 1 et 3 comme plancher et plafond.\nclass Tukey:\n    def fit(self, X):\n        None\n    def outliers(self,x):\n        q1 = np.percentile(x, 25)\n        q3 = np.percentile(x, 75)\n        iqr = q3-q1 \n        floor = q1 - ecart * iqr\n        ceiling = q3 + ecart * iqr\n        outlier_indices = np.where((x < floor)|(x > ceiling))[0]\n        print(len(outlier_indices))\n        return outlier_indices\n\n\nalgo = ('Tukey',Tukey())\nplot_datasets(*algo)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "p44S5D5IJYFm"}, "source": "### Estimation de densit\u00e9 par noyau (KDE)\n\nNon-parametrique, peut aussi capturer les distributions bimodales"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "k_nunpMzJYFm", "trusted": true}, "outputs": [], "source": "from sklearn.neighbors.kde import KernelDensity\n\n# kernel : [\u2018gaussian\u2019|\u2019tophat\u2019|\u2019epanechnikov\u2019|\u2019exponential\u2019|\u2019linear\u2019|\u2019cosine\u2019]\n\nalgo = ('KDE', KernelDensity(bandwidth=0.2,kernel='gaussian'))\n\nplot_datasets(*algo)"}], "metadata": {"PAX": {"revision": 818, "userLang": "fr"}, "celltoolbar": "", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.1"}, "nbTranslate": {"displayLangs": ["*"], "hotkey": "alt-t", "langInMainMenu": true, "sourceLang": "en", "targetLang": "fr", "useGoogleTranslate": true}, "revision": 818, "varInspector": {"cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"delete_cmd_postfix": "", "delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())"}, "r": {"delete_cmd_postfix": ") ", "delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": false}}, "nbformat": 4, "nbformat_minor": 1}