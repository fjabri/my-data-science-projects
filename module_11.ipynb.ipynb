{"cells": [{"cell_type": "markdown", "metadata": {"editable": false}, "source": "---\n# Comparaison des Algorithmes de Classification\n---\n\n<center><img src=\"https://python.gel.ulaval.ca/media/sio-u009/mlprocess_3.png\" alt=\"Processus d'apprentissage automatique\" width=\"50%\"/></center>\n\nUne comparaison va \u00eatre propos\u00e9e entre plusieurs classificateurs de la librairie Python SciKit-Learn sur des ensembels de donn\u00e9es synth\u00e9tiques. Le but de cet exercice est d'illustrer la nature des fronti\u00e8res de d\u00e9cision des diff\u00e9rents algorithmes de classification. Faites attention cependant, les intuitions qui se d\u00e9gagent de ces exemples pourraient ne pas se transferer sur des ensembles de donn\u00e9es r\u00e9\u00e9ls.\n\nEn particulier avec des donn\u00e9es de grande dimensionnalit\u00e9, celles-ci peuvent \u00eatre facilement s\u00e9par\u00e9es lin\u00e9rairement, et des algorithmes simples comme le Naive Bayes ou les SVMs lin\u00e9aires peuvent donner de meilleurs r\u00e9sultats en terme de g\u00e9n\u00e9ralisation que ce que donneraient des algorthmes plus complexes ou avec plus de capacit\u00e9. \n\nLes fonctions pour permettre l'affichage vont pr\u00e9senter les donn\u00e9es d'entrainement comme des points solides annot\u00e9s de leur couleur. Les points de tests seront semi-transparents avec leur couleur \"vraie\" sous la forme d'un 'x'. La valeur en bas \u00e0 droite donne la pr\u00e9cision de l'algorithme sur l'ensemble de donn\u00e9es de test.\n\nsource des exemples : http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "%matplotlib inline\n\n# Code source: Ga\u00ebl Varoquaux\n#              Andreas M\u00fcller\n# Modified for documentation by Jaques Grobler\n# Traduit et d\u00e9compos\u00e9 par Camille Besse\n# License: BSD 3 clause\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nfrom matplotlib.colors import ListedColormap\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared, DotProduct, ConstantKernel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nfg = (24,8)\ncm_points = ListedColormap(['#FF0000','#FFFFFF', '#00FF00','#000000', '#0000FF'])\ncm = 'jet_r'\nparams = {'figure.titlesize': 'xx-large',\n          'font.size': '12',\n          'text.color': 'k',\n          'figure.figsize': fg,\n         }\npylab.rcParams.update(params)\n\n"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "---\n## Ensembles de donn\u00e9es\n---\n\nLes trois ensembles de donn\u00e9es que nous allons utiliser sont les suivants : \n- Les lunes\n- les cercles \n- un ensemble lin\u00e9airement s\u00e9parable\n\nNous pouvons mettre diff\u00e9rent niveaux de bruits dans ces donn\u00e9es. Voyons ce que ca donne."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# Param\u00e8tres initiaux des donn\u00e9es : \n## Pour le g\u00e9n\u00e9rateur de nombre pseu-al\u00e9atoires et la reproductibilit\u00e9 des r\u00e9sultats\nrng_seed = 0 \n## Le bruit dans les donn\u00e9es (entre 0 et 0.5)\nbruit = 0.0\n# ---\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=rng_seed, n_clusters_per_class=2, flip_y=bruit/10,class_sep=2-2*bruit)\nrng = np.random.RandomState(rng_seed)\nX += bruit * 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\nnames = [\"Lunes\", \"Cercles\", \"S\u00e9parables\"]\ndatasets = [make_moons(noise=bruit, random_state=rng_seed),\n            make_circles(noise=bruit/2, factor=0.5, random_state=rng_seed),\n            linearly_separable\n            ]"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Affichons maintenant ces datasets:"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# Cr\u00e9ation des graphiques\nfigure = plt.figure(figsize=fg)\ni = 1\n\n# Pour chaque dataset (compteur, DataSet)\nfor ds_cnt, ds in enumerate(datasets):\n    # Pr\u00e9paration rapide des donn\u00e9es : normalisation des donn\u00e9es et calcul des bornes \n    X, y = ds\n    h = 0.2\n    X = StandardScaler().fit_transform(X)\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    # Visualisons le data set \n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax = plt.subplot(1, len(datasets), i)\n    \n    ax.set_title(names[ds_cnt])\n        \n    # Affichons les ensembles de donn\u00e9es\n    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_points, edgecolors='k')\n\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    ## EXERCICE : Jouez avec les param\u00e8tres de bruit initiaux pour voir les diff\u00e9rences dans les donn\u00e9es"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "D\u00e9finissons un ensemble de fonctions qui vont nous aider \u00e0 visualiser plus simplement les r\u00e9sultats des diff\u00e9rents classificateurs:"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "def creationMesh(X):\n    \"\"\"\n    Cr\u00e9e un grille sur un espace bidimensionnel. Prends le min et le max de chaque dimension et calcule la grille avec une r\u00e9solution de 0.02. \n    X: un vecteur \u00e0 deux colonnes de donn\u00e9es. \n    \"\"\"\n    h = .02  # step size in the mesh\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx,yy\n\n\ndef plotClassifierOnData(name,clf,data,i=3,n=1,multi=False):\n    \"\"\"\n    Pour Afficher les r\u00e9cultat d'un classificateur sur un dataset\n    name : le titre du graphique\n    clf : le classificateur \u00e0 utiliser\n    data : les donn\u00e9es \u00e0 utiliser\n    i : Le i\u00e8me graphique sur n \u00e0 afficher (pour afficher 3 graphiques par ligne)\n    n : Le nombre total de graphiques \u00e0 afficher\n    multi: d\u00e9termine si on affiche juste la fronti\u00e8re de d\u00e9cision (true) ou \n           le score/proba de chaque point de l'espace car on ne peut afficher le score en multiclasse.\n    \"\"\"\n    \n    # Pr\u00e9paration rapide des donn\u00e9es : \n    # normalisation des donn\u00e9es \n    X, y = data\n    X = StandardScaler().fit_transform(X)\n    # S\u00e9paration des donn\u00e9es en TRAIN - TEST\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=rng_seed)\n    # Pour la visualisation des r\u00e9gions et calcul des bornes \n    xx,yy = creationMesh(X)\n\n    # creation du bon nombre de figures \u00e0 afficher (3 par lignes)\n    ax = plt.subplot(n/3,3,i)\n    \n    # entrainement du classificateur et calcul du score final (accuracy)\n    clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n\n    # Pour afficher les fronti\u00e8res de d\u00e9cision on va choisir une color pour \n    # chacun des points x,y du mesh [x_min, x_max]x[y_min, y_max].\n\n    # Si on est en multiclasse (2 ou +) on affiche juste les fronti\u00e8res\n    if multi:\n         Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    else:# sinon on peut afficher le gradient du score\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n    # On affiche le mesh de d\u00e9cision\n    Z = Z.reshape(xx.shape)\n    test = ax.contourf(xx, yy, Z, 100, cmap=cm, alpha=.8)\n\n    #On affiche la l\u00e9gende\n    cbar = plt.colorbar(test)\n    cbar.ax.set_title('score')\n    \n    # On affiche les points d'entrainement\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_points,\n               edgecolors='k',s=100)\n    # Et les points de test\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_points, \n               edgecolors='k',marker='X',s=100)\n\n    # on d\u00e9finit les limites des axes et autres gogosses\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    \n    ax.set_title(name,fontsize=22)\n    # dont le score en bas \u00e0 droite\n    ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n            size=15, horizontalalignment='right')\n\n\ndef plotClassifier(name, clf, datasets):\n    \"\"\"\n    Affiche pour un classificateur donn\u00e9, son r\u00e9sultat sur l'ensemble des datasets pr\u00e9alablement d\u00e9termin\u00e9s\n    name : le nom du classificateur \u00e0 afficher (titre du graphique)\n    clf : un classificateur de scikit-learn\n    datasets : une liste de datasets\n    \"\"\"\n    f = plt.figure(figsize=fg)\n    # Pour chacun des DataSet\n    for ds_cnt, ds in enumerate(datasets):\n        plotClassifierOnData(name, clf, ds,ds_cnt+1,3)\n\n    plt.tight_layout()\n    plt.show()"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "---\n## Algorithmes de Classification\n---\n\nVoyons maintenant les diff\u00e9rents algorithmes et leur(s) param\u00e8tre(s):\n\n1. KNN\n2. Arbre de d\u00e9cision\n3. Random Forest\n4. AdaBoost\n5. Gradient Boosting\n6. R\u00e9gression logistique\n7. R\u00e9seaux de neurones\n8. SVM(s)\n9. Processus Gaussiens\n"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "\n## 1. K-Nearest Neighbors (KNN) \n\nOu les K plus proches voisins. \n\n[`sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights=\u2019uniform\u2019, algorithm=\u2019auto\u2019, leaf_size=30, p=2, metric=\u2019minkowski\u2019, metric_params=None, n_jobs=None)`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n\nLes principaux param\u00e8tres de la classe `sklearn.neighbors.KNeighborsClassifier` sont:\n- `n_neighbors` : le nombre de voisins consid\u00e9r\u00e9s\n- `weights` : `uniform` (tous les poids sont \u00e9gaux), `distance` (le poids est inversement proportionnel \u00e0 la distance par rapport \u00e0 l'\u00e9chantillon test\u00e9) ou toute autre fonction lambda d\u00e9finie par l'utilisateur;\n- `algorithm` : `brute`, `ball_tree`, `KD_tree` ou `auto`. Dans le premier cas, les voisins les plus proches pour chaque cas de test sont calcul\u00e9s par une recherche sur la grille sur l'ensemble d'apprentissage. Dans les deuxi\u00e8me et troisi\u00e8me cas, les distances entre les exemples sont stock\u00e9es dans un arbre pour acc\u00e9l\u00e9rer la recherche des voisins les plus proches. Si vous d\u00e9finissez ce param\u00e8tre sur auto, la bonne fa\u00e7on de trouver les voisins sera automatiquement choisie en fonction du jeu d\u2019entra\u00eenement.\n- `leaf_size` : seuil pour passer \u00e0 la recherche sur la grille si l'algorithme de recherche de voisins est `ball_tree` ou `KD_tree`;\n- `metric` : fonction de calcul de la distance netre les points : `minkowski` (par d\u00e9faut), `manhattan` (si `p`= 1), `euclidean` (si `p`= 2), `chebyshev` (si `p`= Infini). Attention, scilik-learn n'accepte pas p<1.\n\n**Note :** souffre de la mal\u00e9diction de la dimensionnalit\u00e9. Pour des datasets de plusieurs millions de donn\u00e9es dans de grandes dimensions, cf. [Annoy](https://github.com/spotify/annoy) la librairie de Spotify sur les K-NN approxim\u00e9s.\n\n**Exercice :**  En utilisant le constructeur `NeighborsClassifier()`, d\u00e9finissez le mod\u00e8le avec diff\u00e9rentes valeurs d'hyperparam\u00e8tres. Essayer plusieurs combinaisons d'hyperparam\u00e8tres pour comprendre leur influence sur le r\u00e9sultat. N'h\u00e9sitez pas \u00e0 partager vos conclusions sur le forum."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "K = 3\nw = 'uniform' #ou 'distance'\n\n# D\u00e9finir le mod\u00e8le\nmodel = ...\nplotClassifier(\"K Nearest Neighbors\", model, datasets)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "\n## 2. Arbres de Decision\n\n[`sklearn.tree.DecisionTreeClassifier(criterion=\u2019gini\u2019, splitter=\u2019best\u2019, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n\nLes principaux param\u00e8tres de la classe sklearn.tree.DecisionTreeClassifier sont les suivants:\n- `max_depth` : la profondeur maximale de l'arbre;\n- `max_features` : nombre maximal de caract\u00e9ristiques permettant de rechercher la meilleure partition (n\u00e9cessaire avec un grand nombre de caract\u00e9ristiques, car il serait \"co\u00fbteux\" de rechercher des partitions pour toutes les caract\u00e9ristiques);\n- `min_samples_leaf` : nombre minimal d'\u00e9chantillons dans une feuille. Ce param\u00e8tre emp\u00eache la cr\u00e9ation d\u2019arbres o\u00f9 une feuille n\u2019aurait que quelques membres.\n\nLes param\u00e8tres de l\u2019arbre doivent \u00eatre d\u00e9finis en fonction des donn\u00e9es d\u2019entr\u00e9e. Cette op\u00e9ration est g\u00e9n\u00e9ralement effectu\u00e9e au moyen d\u2019une validation crois\u00e9e (vue ce matin).\n\n**Exercice :**  En utilisant le constructeur `DecisionTreeClassifier()`, d\u00e9finissez le mod\u00e8le avec diff\u00e9rentes valeurs d'hyperparam\u00e8tres. Essayer plusieurs combinaisons d'hyperparam\u00e8tres pour comprendre leur influence sur le r\u00e9sultat. N'h\u00e9sitez pas \u00e0 partager vos conclusions sur le forum.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# Profondeur souhait\u00e9e de l'arbre de d\u00e9cision\nprofondeur = 15\n# R\u00e8gles sur une seule ou sur les deux caract\u00e9ristiques \u00e0 la fois\nnombre_var = 1 # 1 ou 2 \n# Nombre minimal d'exemple dans une feuille\nmin_par_feuille = 2\n\n# D\u00e9finir le mod\u00e8le\nmodel = ...\nplotClassifier(\"Arbres de Decision\",  model, datasets)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "\n## 3. For\u00eat Al\u00e9atoire (Random Forest)\n\nUne for\u00eat al\u00e9atoire est un ensemble d'arbres de d\u00e9cision qui peuvent \u00e9ventuellement sur-apprendre chacun diff\u00e9remment (Bagging). \n\n[`sklearn.ensemble.RandomForestClassifier(n_estimators=\u2019warn\u2019, criterion=\u2019gini\u2019, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n\nVoici les param\u00e8tres auxquels nous devons faire attention lorsque nous construisons une for\u00eat :\n- `n_estimators` : le nombre d'arbres dans la for\u00eat;\n- `max_features` : le nombre de caract\u00e9ristiques \u00e0 consid\u00e9rer lors de la recherche du meilleur partage;\n- `min_samples_leaf` : nombre minimal d'\u00e9chantillons requis pour \u00eatre au niveau d'un n\u0153ud feuille;\n- `max_depth` : la profondeur maximale de l'arbre;\n- `criterion` : la fonction utilis\u00e9e pour mesurer la qualit\u00e9 d'une scission.\n\n**Exercice :**  En utilisant le constructeur `RandomForestClassifier()`, d\u00e9finissez le mod\u00e8le avec diff\u00e9rentes valeurs d'hyperparam\u00e8tres. Essayer plusieurs combinaisons d'hyperparam\u00e8tres pour comprendre leur influence sur le r\u00e9sultat. N'h\u00e9sitez pas \u00e0 partager vos conclusions sur le forum."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# Profondeur souhait\u00e9e de l'arbre de d\u00e9cision\nprofondeur = 5\n# Nombres d'arbres dans la for\u00eat\nnombre_arbre = 15\n# R\u00e8gles sur une seule ou sur les deux caract\u00e9ristiques \u00e0 la fois\nnombre_var = 1 # 1 ou 2 \n# Critere de s\u00e9paration : 'gini' ou 'entropy'\ncritere = 'gini'\n# Nombre minimal d'exemple dans une feuille\nmin_par_feuille = 2\n\n# D\u00e9finir le mod\u00e8le\nmodel = ...\nplotClassifier(\"For\u00eat Al\u00e9atoire\",  model, datasets)\n\n"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## 4. AdaBoost\n\nAdaboost utilise \u00e9galement un ensemble d'arbres de d\u00e9cision mais qui sous-apprennent (ils ont une profondeur de 1 par d\u00e9faut).\n\n[`sklearn.ensemble.AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm=\u2019SAMME.R\u2019, random_state=None)`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n\nVoici les param\u00e8tres auxquels nous devons faire attention lorsque nous construisons un mod\u00e8le Adaboost:\n- `n_estimators` : le nombre d'arbres utilis\u00e9s;\n- `learning_rate` : le taux d'apprentissage, i.e. la variation sur le poids des exemples mal class\u00e9s.\n\n**Exercice :**  En utilisant le constructeur `AdaBoostClassifier()`, d\u00e9finissez le mod\u00e8le avec diff\u00e9rentes valeurs d'hyperparam\u00e8tres. Essayer plusieurs combinaisons d'hyperparam\u00e8tres pour comprendre leur influence sur le r\u00e9sultat. N'h\u00e9sitez pas \u00e0 partager vos conclusions sur le forum."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# Nombres d'arbres boosters\nnombre_arbre = 10\n# taux d'apprentissage \nalpha = 0.5\n\n# D\u00e9finir le mod\u00e8le\nmodel = ...\nplotClassifier(\"AdaBoost\", model, datasets)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## 5. Gradient Boosting\n\nGradient Boosting est une version g\u00e9n\u00e9ralis\u00e9e d'Adaboost. \n\n[`sklearn.ensemble.GradientBoostingClassifier(loss=\u2019deviance\u2019, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=\u2019friedman_mse\u2019, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort=\u2019auto\u2019, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n\nLes param\u00e8tres auxquels nous devons faire attention sont les m\u00eames que lorsque nous construisons un mod\u00e8le Adaboost :\n- `n_estimators` : le nombre d'arbres utilis\u00e9s;\n- `learning_rate` : le taux d'apprentissage, i.e. la variation sur le poids des exemples mal class\u00e9s;\n\nAuxquels ont peut ajouter \n- `subsample` : La fraction d'examples \u00e0 utiliser pour ajuster les classificateurs (`bootstraping`). Si inf\u00e9rieur \u00e0 1.0, l'algorithme devient du [`Stochastic Gradient Boosting`](https://en.wikipedia.org/wiki/Gradient_boosting#Stochastic_gradient_boosting).\n\n**Exercice :**  En utilisant le constructeur `GradientBoostingClassifier()`, d\u00e9finissez le mod\u00e8le avec diff\u00e9rentes valeurs d'hyperparam\u00e8tres. Essayer plusieurs combinaisons d'hyperparam\u00e8tres pour comprendre leur influence sur le r\u00e9sultat. N'h\u00e9sitez pas \u00e0 partager vos conclusions sur le forum."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# Nombres d'arbres boosters\nnombre_arbre = 15\n# taux d'apprentissage \nalpha = 0.1\n# Profondeur souhait\u00e9e de l'arbre de d\u00e9cision\nprofondeur = 5\n# fraction des examples\nfraction = 1.0\n\n# D\u00e9finir le mod\u00e8le\nmodel = ...\nplotClassifier(\"Boosting de gradient\",  model, datasets)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## 6. Regression logistique\n\nOu comment faire de la r\u00e9gression lin\u00e9aire avec des classes.\n\n[`sklearn.linear_model.LogisticRegression(penalty=\u2019l2\u2019, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=\u2019warn\u2019, max_iter=100, multi_class=\u2019warn\u2019, verbose=0, warm_start=False, n_jobs=None)`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n\nLes principaux param\u00e8tres de la r\u00e9gression logistique sont les suivants:\n- `penalty` : La norme ($L_1$ ou $L_2$) utilis\u00e9e pour calculer la fonction de perte;\n- `max_iter` : nombre maximal d'it\u00e9ration ;\n- `C` : Le param\u00e8tre inverse de r\u00e9gularisation dans $]0,1]$, plus `C` est proche de 0, plus la r\u00e9gularisation est forte.\n\nOn constate que dans le cas de ces datasets, la capacit\u00e9 simple du classificateur n'est pas suffisante, il faudrait faire une projection dans l'espace des caract\u00e9ristiques pour augmenter sa capacit\u00e9 et ainsi am\u00e9liorer l'exactitude.\n\n**Exercice :**  En utilisant le constructeur `LogisticRegression()`, d\u00e9finissez le mod\u00e8le avec diff\u00e9rentes valeurs d'hyperparam\u00e8tres. Essayer plusieurs combinaisons d'hyperparam\u00e8tres pour comprendre leur influence sur le r\u00e9sultat. N'h\u00e9sitez pas \u00e0 partager vos conclusions sur le forum.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# p\u00e9nalit\u00e9 = 'l1' ou 'l2'\np\u00e9nalit\u00e9 = 'l2' # ou 'l1'\n# Nombre d'it\u00e9rations pour s'assurer de la convergence, si un warning apparait, c'est g\u00e9n\u00e9ralement \u00e0 cause de ca.\niterations = 100\n# R\u00e9gularisation inverse ]0,1]\nireg = 1\n\n# D\u00e9finir le mod\u00e8le\nmodel = ...\nplotClassifier(\"Regression Logistique\", model, datasets)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## 7. Perceptron Multi-Couches\n\n[`sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), activation=\u2019relu\u2019, solver=\u2019adam\u2019, alpha=0.0001, batch_size=\u2019auto\u2019, learning_rate=\u2019constant\u2019, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)`](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n\nLes principaux param\u00e8tres du perceptron sont les suivants:\n- `hidden_layer_sizes` : Le nombre de neurones sur la (ou les) couche(s) cach\u00e9e(s).\n- `activation` : La fonction d'activation de chaque neurone.\n- `alpha` : Taux de r\u00e9gularisation $L_2$ sur les poids.\n- `max_iter`,`tol` : `max_iter` est le nombre d'it\u00e9ration avant d'arr\u00eater si on ne souhaite pas attendre la convergence d\u00e9finie par la tol\u00e9rance `tol` ( = 0.0001 par d\u00e9faut).\n\n\n**Exercice :**  En utilisant le constructeur `MLPClassifier()`, d\u00e9finissez le mod\u00e8le avec diff\u00e9rentes valeurs d'hyperparam\u00e8tres. Essayer plusieurs combinaisons d'hyperparam\u00e8tres pour comprendre leur influence sur le r\u00e9sultat. N'h\u00e9sitez pas \u00e0 partager vos conclusions sur le forum."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# Taux de r\u00e9gularisation (defaut \u00e0 0.0001)\nregu = 0.01\n# Nombre d'it\u00e9rations pour s'assurer de la convergence, si un warning apparait, c'est g\u00e9n\u00e9ralement \u00e0 cause de ca.\niterations = 2000\n\n# Nombre de neurones sur la (ou les) couche(s) cach\u00e9e(s), une valeur par couche. eg. (5,) : une seule couche cach\u00e9e de 5 neurones. (5,3) : deux couches cach\u00e9es une avec 5 l'autre avec 3.\ntaille = (5,)\n# Fonction d'activation : \u2018identity\u2019, \u2018logistic\u2019, \u2018tanh\u2019, \u2018relu\u2019\nactiv = 'relu'\n\n# D\u00e9finir le mod\u00e8le\nmodel = ...\nplotClassifier(\"Perceptron Multi-Couches\",  model, datasets)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## 8. Support Vector Machines (SVM)\n\nOu machines \u00e0 vecteur de support (avec noyau lin\u00e9aire ou gaussien).\n\n[`sklearn.svm.SVC(C=1.0, kernel=\u2019rbf\u2019, degree=3, gamma=\u2019auto_deprecated\u2019, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=\u2019ovr\u2019, random_state=None)`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n\nLes principaux param\u00e8tres du SVM sont les suivants:\n- `kernel` : le noyau \u00e0 utiliser pour la mesure de distance;\n- `C` : r\u00e9gularisation des points de support;\n- `max_iter`,`tol` : `max_iter` est le nombre d'it\u00e9ration avant d'arr\u00eater si on ne souhaite pas attendre la convergence d\u00e9finie par la tol\u00e9rance `tol` ( = 0.001 par d\u00e9faut).\n\n**Exercice :**  En utilisant le constructeur `MLPClassifier()`, d\u00e9finissez le mod\u00e8le avec diff\u00e9rentes valeurs d'hyperparam\u00e8tres. Essayer plusieurs combinaisons d'hyperparam\u00e8tres pour comprendre leur influence sur le r\u00e9sultat. N'h\u00e9sitez pas \u00e0 partager vos conclusions sur le forum."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# Taux de r\u00e9gularisation (defaut \u00e0 0.0001)\nregu= .1\n# Noyau utilis\u00e9 pour la transformation d'espace : 'linear', 'poly' (de degr\u00e9 3 par d\u00e9faut), 'rbf', 'sigmoid'\nnoyau = 'rbf'\n# Nombre d'it\u00e9rations pour s'assurer de la convergence, si un warning apparait, c'est g\u00e9n\u00e9ralement \u00e0 cause de ca.\niterations = 2000\n\n# D\u00e9finir le mod\u00e8le\nmodel = ...\nplotClassifier(\"SVM avec noyau \"+noyau, model, datasets)\n"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## 9. Processus Gaussien\n\n[`sklearn.gaussian_process.GaussianProcessClassifier(kernel=None, optimizer=\u2019fmin_l_bfgs_b\u2019, n_restarts_optimizer=0, max_iter_predict=100, warm_start=False, copy_X_train=True, random_state=None, multi_class=\u2019one_vs_rest\u2019, n_jobs=None)`](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html)\n\nLe principal param\u00e8tre du Processus Gaussien est le `kernel` i.e. le noyau \u00e0 utiliser pour la mesure de distance. On pourra essayer les diff\u00e9rents noyaux d\u00e9finis dans le code.\nIl ne sera pas n\u00e9cessaire d'optimiser les hyperparma\u00e8tres du noyau puisque l'apprentissage s'en occupe.\n\nVoici la documentation des diff\u00e9rents noyaux : \n- [Constant](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ConstantKernel.html#sklearn.gaussian_process.kernels.ConstantKernel)\n- [Dot product](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.DotProduct.html#sklearn.gaussian_process.kernels.DotProduct)\n- [RBF](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF)\n- [Matern](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html#sklearn.gaussian_process.kernels.Matern) : RBF g\u00e9n\u00e9ralis\u00e9\n- [Rational Quadratic](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RationalQuadratic.html#sklearn.gaussian_process.kernels.RationalQuadratic) : mixture infinie de RBF\n\n**Exercice :**  En utilisant le constructeur `GaussianProcessClassifier()`, d\u00e9finissez le mod\u00e8le avec diff\u00e9rentes valeurs d'hyperparam\u00e8tres. Essayer plusieurs combinaisons d'hyperparam\u00e8tres pour comprendre leur influence sur le r\u00e9sultat. N'h\u00e9sitez pas \u00e0 partager vos conclusions sur le forum."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# noyauRBF_isotropique = 1.0 * RBF([1.0])\n# noyauRBF_anisotropique = 1.0 * RBF([1.0, 1.0])\n\nnoyauRBF = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))\nnoyauRQ = 1.0 * RationalQuadratic(length_scale=1.0, alpha=0.1)\nnoyauDP = ConstantKernel(0.1, (0.01, 10.0)) * (DotProduct(sigma_0=1.0, sigma_0_bounds=(0.1, 10.0)) ** 2)\nnoyauM = 1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0), nu=1.0)\n\n# D\u00e9finir le mod\u00e8le\nmodel = ...\nplotClassifier(\"Processus Gaussien\",  model, datasets)\n"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## A vous de jouer\n\nEn utilisant le jeu de donn\u00e9es Titanic, d\u00e9terminez quel mod\u00e8le est le plus adapt\u00e9 pour pr\u00e9dire la survie d'un passager.\nGardez \u00e0 l'esprit : \n\n1. C'est de la classification binaire (oui/non)\n2. Les donn\u00e9es n'ont pas \u00e9t\u00e9 pr\u00e9par\u00e9es (cf. [Module 0](./Module_0_Coll_Prep.ipynb))\n3. L'ensemble de donn\u00e9es est d\u00e9ja d\u00e9coup\u00e9 entrainement/test\n4. Il vous faudra valider parmi plusieurs mod\u00e8les avec plusieurs hyperparam\u00e8tres, il vous faut donc un ensemble de validation (ou bien plusieurs si vous souhaitez faire de la validation crois\u00e9e)\n\nDonnez ensuite sur le forum quel a \u00e9t\u00e9 le meilleur mod\u00e8le appris, quels sont les hyperparam\u00e8tres qui ont \u00e9t\u00e9 choisis de mani\u00e8re \u00e0 ce que d'autres participans puissent reproduire vos r\u00e9sultats. \n\n\u00c9vitez de donner du code sur le forum pour laisser la possibilit\u00e9 \u00e0 chacun de trouver par soi-m\u00eame ;-) "}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "import wget\n \n!rm './titanic_train.csv' './titanic_test.csv'\n\nwget.download('https://raw.githubusercontent.com/iid-ulaval/EEAA-datasets/master/titanic_train.csv','./titanic_train.csv')\nwget.download('https://raw.githubusercontent.com/iid-ulaval/EEAA-datasets/master/titanic_test.csv','./titanic_test.csv')"}], "metadata": {"PAX": {"revision": 804, "userLang": "fr"}, "celltoolbar": "", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.1"}, "revision": 804, "varInspector": {"cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"delete_cmd_postfix": "", "delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())"}, "r": {"delete_cmd_postfix": ") ", "delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": false}}, "nbformat": 4, "nbformat_minor": 2}