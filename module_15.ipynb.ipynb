{"cells": [{"cell_type": "markdown", "metadata": {"editable": false}, "source": "---\n# \u00c9valuation des mod\u00e8les de classification\n---\n\n<center><img src=\"https://python.gel.ulaval.ca/media/sio-u009/mlprocess_4.png\" alt=\"Processus d'apprentissage automatique\" width=\"50%\"/></center>\n\nDans cette s\u00e9quence nous allons \u00e9valuer, \u00e0 l'aide des diff\u00e9rentes m\u00e9triques vues dans le cours, diff\u00e9rents mod\u00e8les de classification.\n\nDans cette s\u00e9quence nous allons repartir de l'exemple simple vu dans le module de classification ([`make_moons`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html)) et \u00e9tablir un mod\u00e8le de r\u00e9f\u00e9rence de base auquel se comparer par la suite. Dans un second temps nous allons \u00e9valuer le mod\u00e8le de votre choix sur cet ensemble. \n\nDans la prochaine s\u00e9quence nous passerons \u00e0 des donn\u00e9es r\u00e9elles (donc multi-classes).\n\nImportons d'abord les librairies n\u00e9cessaires.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.datasets import load_digits, make_moons, make_blobs\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve, fbeta_score, log_loss, plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve, matthews_corrcoef\nfrom imblearn.datasets import make_imbalance\nimport matplotlib.pylab as pylab\nfrom matplotlib.colors import ListedColormap\nimport itertools\nfrom matplotlib import pyplot"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "fg = (8,6)\ncm_points = ListedColormap(['#FF0000','#FFFFFF', '#00FF00','#000000', '#0000FF'])\ncm = plt.cm.viridis\nparams = {'figure.titlesize': 'xx-large',\n          'font.size': '12',\n          'text.color': 'k',\n          'figure.figsize': fg,\n         }\npylab.rcParams.update(params)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Ici, nous avons copi\u00e9-coll\u00e9 la fonction utilis\u00e9e dans la module de pr\u00e9sentation des alhgos de classification."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "def plotClassifierOnData(name,clf,data,threshold=None):\n    \"\"\"\n    Pour Afficher les r\u00e9sultat d'un classificateur sur un dataset\n    name : le titre du graphique\n    clf : le classificateur \u00e0 utiliser\n    data : les donn\u00e9es \u00e0 utiliser\n    i : Le i\u00e8me graphique sur n \u00e0 afficher (pour afficher 3 graphiques par ligne)\n    n : Le nombre total de graphiques \u00e0 afficher\n    multi: d\u00e9termine si on affiche juste la fronti\u00e8re de d\u00e9cision (true) ou \n           le score/proba de chaque point de l'espace car on ne peut afficher le score en multiclasse.\n    \"\"\"\n    \n    f = plt.figure(figsize=fg)\n    \n    # Pr\u00e9paration rapide des donn\u00e9es : normalisation des donn\u00e9es et calcul des bornes \n    X, y = data\n    \n    # Visualisation des r\u00e9gions\n    h = .02  # step size in the mesh\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    ax = plt.subplot(1,1,1)\n    \n    # Pour afficher les fronti\u00e8res de d\u00e9cision on va choisir une color pour \n    # chacun des points x,y du mesh [x_min, x_max]x[y_min, y_max].\n\n    # Si on est en multiclasse (2 ou +) on affiche juste les fronti\u00e8res\n    if threshold:\n         Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) > threshold\n    else:# sinon on peut afficher le gradient du score\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n    # On affiche le mesh de d\u00e9cision\n    Z = Z.reshape(xx.shape)\n    test = ax.contourf(xx, yy, Z, 100, cmap=cm, alpha=.8)\n\n    #On affiche la l\u00e9gende\n    cbar = plt.colorbar(test)\n    cbar.ax.set_title('score')\n    \n    # On affiche les points d'entrainement\n    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_points,\n               edgecolors='k',s=100)\n\n\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    \n    ax.set_title(name,fontsize=22)\n    \n    #plt.tight_layout()\n    plt.show()   "}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Initialisons l'ensemble de donn\u00e9es avec significativement plus de points de mani\u00e8re \u00e0 cr\u00e9er une forme de d\u00e9s\u00e9quilibre \u00e0 l'int\u00e9rieur. \n\nN'h\u00e9sitez pas \u00e0 jouer avec ce d\u00e9s\u00e9quilibre et le bruit du dataset pour voir les diff\u00e9rents effets sur la suite de l'exemple !"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# Param\u00e8tres du dataset\nN = 100000;\nratio_debalancement = 0.01;\nbruit=0.3\n\n# G\u00e9n\u00e9rations des donn\u00e9es\nX, y = make_moons(n_samples=10*N, shuffle=True, noise=bruit, random_state=42)\nX, y =  make_imbalance(X,y, sampling_strategy={0: int(N*(1-ratio_debalancement)), 1: int(N*ratio_debalancement)})\n\n# Visualisation\nf = plt.figure() \nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_points, edgecolor='k', s=10, alpha=.5)\nplt.show()"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## S\u00e9paration des donn\u00e9es train/validation/test 40% / 30% / 30%"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.30, stratify=y)\nX_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.42857, stratify=y_)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## \u00c9tablir le baseline de performance\n\nPour ce mod\u00e8le de r\u00e9f\u00e9rence nous allons utiliser le classificateur stupide qui utilise la classe majoritaire : [`DummyClassifier(strategy='most_frequent',...)`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# baseline_clf = ..."}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Et on en mesure l'exactitude comme borne inf\u00e9rieure sur la classification :"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "y_pred_train = baseline_clf.predict(X_train)\nacc_baseline = accuracy_score(y_pred_train, y_train)\nprint('Performance baseline: ', acc_baseline)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## Entra\u00eenement d'un classificateur\n\nA partir de la vous pouvez choisir n'importe quel classificateur et l'entrainer tel que nous les avons essay\u00e9 dans le module 3.\nNous l'\u00e9valuerons par la suite avec sa matrice de confusion plutot que par juste l'eactitude afin d'en tirer le plus de m\u00e9triques possibles.\n\nVous trouverez un rappel des classifficateurs disponible dans [Scikit-Learn ici](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning).\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# clf = ..."}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## Calculer la matrice de confusion sur l'ensemble de validation\n\nA partir de notre classificateur entrain\u00e9, il est possible de d\u00e9terminer quelle probabilit\u00e9 il donne \u00e0 la classe `1` (ou la classe `0`) en utilisant la fonction `predict_proba()` de la plupart des classificateurs de SciKit-Learn. Un exemple vous est donn\u00e9 ci-dessous.\n\nUne fois la probabilit\u00e9 calcul\u00e9e, il est possible de se ramener \u00e0 une classe en choisissant un seuil. Dans l'exemple ci-dessous nous avons utilis\u00e9 $0.5$, qui est la valeur de seuil par d\u00e9faut."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "threshold = .5\n\ny_val_pred_proba = clf.predict_proba(X_val)[:,1]\ny_val_pred_label = y_val_pred_proba > threshold"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Avec le seuil choisi, on peut maintenant calculer la matrice de confusion de notre classificateur en utilisant la fonction [`plot_confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "f = plt.figure(figsize=(4,4))\ncnf_matrix = confusion_matrix(y_val, y_val_pred_label)\nplot_confusion_matrix(clf, X_val, y_val, values_format='1')"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "et aussi calculer l'exactitude sur l'ensemble de validation avec la fonction [`accuracy_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "accuracy_score(y_val, y_val_pred_label)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Cependant, est ce que le choix de $0.5$ pour le seuil \u00e9tait le meilleur ? Si on met un autre seuil, ne peut on mieux g\u00e9n\u00e9raliser sur l'ensemble de validation ? \n\nEssayez de trouver un meilleur seuil, soit \u00e0 la mitaine, soit en regroupant toutes ces cellules dans une seule et en utilisant un peu de programmation pour voir quel seuil serait le meilleur ;-)\n\nN'h\u00e9sitez pas \u00e0 \u00e9changer sur le forum \u00e0 ce sujet !"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## Calculer la courbe ROC et l'AUC\n\nVous avez vu dans la partie th\u00e9orique qu'il existe en fait un mani\u00e8re de tester tous les seuils d'un coup avec la [courbe ROC](https://fr.wikipedia.org/wiki/Courbe_ROC) ([`roc_curve()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) dans Scikit-Learn). C'est ce que nous allons faire maintenant."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "y_proba_val = clf.predict_proba(X_val)[:, 1]\nfpr_rf, tpr_rf, thresholds = roc_curve(y_val, y_proba_val)\n\nfig, ax = plt.subplots()\nplot_roc_curve(clf, X_val, y_val, name='ROC curve', ax=ax)\nax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n        label='Chance', alpha=.8)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "roc_auc_score(y_val, y_proba_val)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "thresholds[np.argmax(-fpr_rf + tpr_rf)]"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "plotClassifierOnData(\"clf\",clf,(X_val, y_val))"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## Calculer la courbe de pr\u00e9cision/rappel\n\nIl est \u00e9galement possible de le faire avec la courbe de [pr\u00e9cision/rappel](https://fr.wikipedia.org/wiki/Pr%C3%A9cision_et_rappel) ([`precision_recall_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) dans Scikit-Learn).\n\nOn peut ainsi soit calculer toutes les pr\u00e9cisions et rappels possible selon le seuil choisi : "}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "precision, recall, thresholds = precision_recall_curve(y_val, y_proba_val)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "ou bien afficher directement la courbe de pr\u00e9cision/rappel : "}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "plot_precision_recall_curve(clf, X_val, y_val)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Dans certaines applications, le rappel est beaucoup plus important que la pr\u00e9cision. Par exemple, lorsqu\u2019il s\u2019agit de trouver les courriels qui ne sont pas des pourriels, il est tr\u00e8s important de trouver tous les courriels qui ne sont pas des pourriels ; il est cependant moins grave que certains pourriels survivent au filtrage.\n\nLe contraire est parfois vrai. Supposons qu\u2019on doive attribuer \u00e0 des documents des mots-cl\u00e9s pour faciliter la recherche. On peut mesurer la qualit\u00e9 d\u2019ex\u00e9cution de cette t\u00e2che en fonction du rappel (est-ce qu\u2019on a trouv\u00e9 tous les mots-cl\u00e9s qui s\u2019appliquent ?) et de la pr\u00e9cision (est-ce que tous les mots-cl\u00e9s attribu\u00e9s sont pertinents ?). Dans ce cas, la pr\u00e9cision n\u2019est pas tr\u00e8s importante, mais on souhaite que tous les documents puissent \u00eatre trait\u00e9s.\n\nComment choisir le meilleur compromis lorsque la pr\u00e9cision et le rappel sont pratiquement d\u2019\u00e9gale importance ? Une des m\u00e9thodes utilis\u00e9es est de maximiser la moyenne harmonique de la pr\u00e9cision et du rappel : $\\frac{r+p}{2rp}$. On appelle cette moyenne le score F."}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## \u00c9valuer le [F-score](https://fr.wikipedia.org/wiki/Pr%C3%A9cision_et_rappel#F-mesure) pour diff\u00e9rents seuils\n\nPour rappel doinc, la mesure qui combine la pr\u00e9cision et le rappel est leur moyenne harmonique, nomm\u00e9e F-mesure ou F-score :\n$F=2\\cdot \\frac{({\\text{pr\u00e9cision}}\\cdot {\\text{rappel}})}{({\\text{pr\u00e9cision}}+{\\text{rappel}})}$\n\nElle est \u00e9galement connue sous le nom de mesure $F_1$, car pr\u00e9cision et rappel sont pond\u00e9r\u00e9s de fa\u00e7on \u00e9gale. Il s'agit d'un cas particulier de la mesure g\u00e9n\u00e9rale $F_{\\beta}$ (pour des valeurs r\u00e9elles positives de $\\beta$ : \n\n$F_{\\beta }={\\frac {(1+\\beta ^{2})\\cdot ({\\text{pr\u00e9cision}}\\cdot {\\text{rappel}})}{(\\beta ^{2}\\cdot {\\text{pr\u00e9cision}}+{\\text{rappel}})}}$\n\nCalculons donc maintenant le score F pour tous les seuils possibles. N'h\u00e9sitez pas \u00e0 voir l'impact de $\\beta$ sur la courbe r\u00e9sultat."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "beta = 1\n\nM = 1000\nthresholds = np.linspace(-0.1, 1.1, M)\nscore = np.zeros(shape=(M,))\nfor i in range(0,len(thresholds)):\n    y_pred_ = (y_proba_val > thresholds[i])*1.0\n    score[i] = fbeta_score(y_val, y_pred_, beta)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "f = plt.figure()\n\nplt.plot(thresholds, score)\nplt.xlabel('Threshold')\nplt.ylabel('F{0}-Score'.format(beta))\nplt.title('F{0}-Score'.format(beta))\nplt.show()"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Et pour trouver le seuil qui maximise la F-#mesure choisie : "}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "thresholds[np.argmax(score)]"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## Calculer le [coefficient de corr\u00e9lation de Mathews (MCC)](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient)\n\nUn des d\u00e9fauts du F-score est sa faiblesse face aux donn\u00e9es d\u00e9balanc\u00e9es. En voici un exemple donn\u00e9 sur la page de wikipedia ci-dessus : \n\nComme expliqu\u00e9 par Davide Chicco dans son article \"Ten quick tips for machine learning in computational biology\" (BioData Mining, 2017) et par Giuseppe Jurman dans son article \"The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation\" (BMC Genomics, 2020), le coefficient de corr\u00e9lation de Matthews est plus instructif que le score $F_1$ et plus pr\u00e9cis dans l'\u00e9valuation des probl\u00e8mes de classification binaire, car il tient compte des rapports d'\u00e9quilibre des quatre cat\u00e9gories de la matrice de confusion (vrais positifs, vrais n\u00e9gatifs, faux positifs, faux n\u00e9gatifs).\n\nAinsi, afin d'avoir une compr\u00e9hension globale d'une pr\u00e9diction, si vous d\u00e9cidez de tirer profit des scores statistiques communs, tels que l'exactitude et le score $F_1$.\n\n${\\text{accuracy}}={\\frac {TP+TN}{TP+TN+FP+FN}}$ (\u00c9quation 1, pr\u00e9cision : la pire valeur = 0 ; la meilleure valeur = 1)\n\n${\\text{F1 score}}={\\frac {2TP}{2TP+FP+FN}}$ (\u00c9quation 2, score F1 : la pire valeur = 0 ; la meilleure valeur = 1)\n\nToutefois, m\u00eame si la pr\u00e9cision et le score $F_1$ sont largement utilis\u00e9s en statistique, ils peuvent tous deux \u00eatre trompeurs, car ils ne tiennent pas pleinement compte de la taille des quatre classes de la matrice de confusion dans le calcul du score final.\n\nSupposons, par exemple, que vous ayez un ensemble de validation tr\u00e8s d\u00e9s\u00e9quilibr\u00e9 compos\u00e9 de 100 \u00e9l\u00e9ments, dont 95 sont des \u00e9l\u00e9ments positifs et seulement 5 sont des \u00e9l\u00e9ments n\u00e9gatifs. Et supposons aussi que vous ayez fait quelques erreurs dans la conception et l'apprentissage de votre classificateur d'apprentissage machine, et que vous ayez maintenant un algorithme qui pr\u00e9dit toujours le positif. Imaginez que vous n'\u00eates pas conscient de ce probl\u00e8me.\n\nEn appliquant votre seul pr\u00e9dicteur positif \u00e0 votre ensemble de validation d\u00e9s\u00e9quilibr\u00e9, vous obtenez donc des valeurs pour les cat\u00e9gories de la matrice de confusion :\n\nTP = 95, FP = 5 ; TN = 0, FN = 0.\n\nCes valeurs conduisent aux scores de performance suivants : exactitude = 95%, et score $F_1$ = 97,44%. En lisant ces scores trop optimistes, vous serez alors tr\u00e8s heureux et penserez que votre algorithme d'apprentissage automatique fait un excellent travail. Il est \u00e9vident que vous feriez fausse route.\n\nAu contraire, pour \u00e9viter ces dangereuses illusions trompeuses, il existe un autre score de performance que vous pouvez exploiter : le coefficient de corr\u00e9lation de Matthews (MCC).\n\n$ \\text{MCC}= \\frac{TP\\times TN-FP\\times FN}{\\sqrt {(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$ (\u00c9quation 3, MCC : la pire valeur = -1 ; la meilleure valeur = +1).\n\nEn consid\u00e9rant la proportion de chaque classe de la matrice de confusion dans sa formule, son score n'est \u00e9lev\u00e9 que si votre classificateur obtient de bons r\u00e9sultats \u00e0 la fois sur les \u00e9l\u00e9ments n\u00e9gatifs et positifs.\n\nDans l'exemple ci-dessus, le score MCC serait *ind\u00e9fini* (puisque TN et FN seraient 0, donc le d\u00e9nominateur de l'\u00e9quation 3 serait 0). En v\u00e9rifiant cette valeur, au lieu de la pr\u00e9cision et de du score $F_1$, vous pourriez alors remarquer que votre classificateur va dans la mauvaise direction, et vous prendriez conscience qu'il y a des probl\u00e8mes \u00e0 r\u00e9soudre avant de poursuivre.\n\nPrenons cet autre exemple. Vous avez effectu\u00e9 une classification sur le m\u00eame ensemble de donn\u00e9es, ce qui a donn\u00e9 les valeurs suivantes pour les cat\u00e9gories de la matrice de confusion :\n\nTP = 90, FP = 4 ; TN = 1, FN = 5.\n\nDans cet exemple, le classificateur a obtenu de bons r\u00e9sultats en classant les cas positifs, mais n'a pas pu reconna\u00eetre correctement les \u00e9l\u00e9ments de donn\u00e9es n\u00e9gatifs. L\u00e0 encore, le score $F_1$ et les scores d'exactitude obtenus seraient extr\u00eamement \u00e9lev\u00e9s : exactitude = 91%, et score $F_1$ = 95,24%. Comme dans le cas pr\u00e9c\u00e9dent, si un chercheur n'analysait que ces deux indicateurs de score, sans tenir compte du MCC, il penserait \u00e0 tort que l'algorithme est assez performant dans sa t\u00e2che, et aurait l'illusion de r\u00e9ussir.\n\nD'autre part, la v\u00e9rification du coefficient de corr\u00e9lation de Matthews serait une fois de plus essentielle. Dans cet exemple, la valeur du MCC serait de 0,14 (\u00e9quation 3), ce qui indique que l'algorithme fonctionne de la m\u00eame mani\u00e8re que la conjecture al\u00e9atoire. Agissant comme une alarme, le MCC serait en mesure d'informer le praticien du data mining que le mod\u00e8le statistique est peu performant.\n\nPour ces raisons, nous encourageons vivement \u00e0 \u00e9valuer les performances de chaque test par le biais du coefficient de corr\u00e9lation de Matthews (MCC), au lieu de la pr\u00e9cision et du score $F_1$, pour tout probl\u00e8me de classification binaire.\n    - Davide Chicco, \"Ten quick tips for machine learning in computational biology\" (BioData Mining, 2017)\n\nNotez que le score $F_1$ d\u00e9pend de la classe qui est d\u00e9finie comme \u00e9tant la classe positive. Dans le premier exemple ci-dessus, le score $F_1$ est \u00e9lev\u00e9 parce que la classe majoritaire est d\u00e9finie comme la classe positive. L'inversion des classes positives et n\u00e9gatives donne la matrice de confusion suivante :\n\nTP = 0, FP = 0 ; TN = 5, FN = 95\n\nCela donne un score de $F_1 = 0\\%$.\n\nLe MCC ne d\u00e9pend pas de la classe positive, qui a l'avantage par rapport \u00e0 la note $F_1$ d'\u00e9viter de d\u00e9finir incorrectement la classe positive. \n\nCalculons le maintenant pour notre exemple d'origine.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "M = 1000\nmcc = np.zeros(shape=(M,))\nfor i in range(0,len(thresholds)):\n    y_pred_ = (y_proba_val > thresholds[i])*1.0\n    mcc[i] = matthews_corrcoef(y_val, y_pred_)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "f = plt.figure()\n\nplt.plot(thresholds, mcc)\nplt.xlabel('Threshold')\nplt.ylabel('MCC')\nplt.title('MCC')\nplt.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "thresholds[np.argmax(mcc)]"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## Calcul de la m\u00e9trique d'[entropie crois\u00e9e](https://fr.wikipedia.org/wiki/Entropie_crois%C3%A9e)\n\nLa perte d'entropie crois\u00e9e, ou perte logarithmique ([`log_loss`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html)), mesure la performance d'un mod\u00e8le de classification dont la sortie est une valeur de probabilit\u00e9 comprise entre 0 et 1. La perte d'entropie crois\u00e9e augmente \u00e0 mesure que la probabilit\u00e9 pr\u00e9vue s'\u00e9carte de l'\u00e9tiquette r\u00e9elle. Ainsi, pr\u00e9dire une probabilit\u00e9 de 0,012 lorsque l'\u00e9tiquette d'observation r\u00e9elle est de 1 serait mauvais et entra\u00eenerait une valeur de perte \u00e9lev\u00e9e. Un mod\u00e8le parfait aurait une perte logarithmique de 0.\n\nDans le cadre de notre exemple : "}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "log_loss(y_val, y_proba_val)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## Calcul de la m\u00e9trique d'utilit\u00e9\n\nLa m\u00e9trique d'utilit\u00e9 est consid\u00e9r\u00e9e la plus flexible d'entre toutes, car elle permet d'\u00e9valuer pr\u00e9cis\u00e9ment le poid de chaque type d'erreur et de r\u00e9compenser comme voulu chaque type de r\u00e9ussite. Vous avez vu des exemples dans la capsule th\u00e9orique et je vous invite \u00e0 jouer avec les valeurs ci dessous pour voir l'impact sur la m\u00e9trique de votre clasiificateur qui a \u00e9t\u00e9 optimis\u00e9 sur l'exactitude si vous n'avez pas sp\u00e9cifi\u00e9 de fonction de perte lors de l'entrainement :-)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "FP_loss = -0\nFN_loss = -0\nTP_reward = 1\nTN_reward = 1\nutility = lambda cnf_matrix: cnf_matrix[1,1]*TP_reward + cnf_matrix[0,0]*TN_reward + cnf_matrix[1,0]*FN_loss + cnf_matrix[0,1]*FP_loss"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "M = 100\nthresholds = np.linspace(-0.1, 1.1, M)\nscore = np.zeros(shape=(M,))\nfor i in range(0,len(thresholds)):\n    y_pred_ = (y_proba_val > thresholds[i])*1.0\n    cnf_matrix = confusion_matrix(y_val, y_pred_)\n    score[i] = utility(cnf_matrix)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "f = plt.figure()\npyplot.grid(True)\npyplot.ion()\npyplot.yscale('symlog')\npyplot.plot(thresholds, score)\npyplot.xlabel('Threshold')\npyplot.ylabel('utility')\npyplot.title('Utility-based cost')\npyplot.ylim(29000, 30000)\npyplot.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "thresholds[np.argmax(score)]"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Rendu outill\u00e9 par toutes ces m\u00e9triques, essayez diff\u00e9rents mod\u00e8les avec diff\u00e9rentes fonctions de pertes pour voir comment ils se comportent vis-\u00e0 vis de m\u00e9triques potentiellement non diff\u00e9rentiables."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": ""}], "metadata": {"PAX": {"revision": 819, "userLang": "fr"}, "celltoolbar": "", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.1"}, "revision": 819}, "nbformat": 4, "nbformat_minor": 2}