{"cells": [{"cell_type": "markdown", "metadata": {"editable": false}, "source": "---\n# \u00c9valuation des mod\u00e8les de classification : Exemple (un peu) plus r\u00e9aliste\n---\n\n<center><img src=\"https://python.gel.ulaval.ca/media/sio-u009/mlprocess_4.png\" alt=\"Processus d'apprentissage automatique\" width=\"50%\"/></center>\n\nDans cette s\u00e9quence nous allons repartir d'un ensemble de donn\u00e9es beaucoup plus complexe \u00e0 base d'images de chiffres manuscrits dont le but est d'identifier le chiffre \u00e9crit. C'est un probl\u00e8me multiclasse (10 chiffres de 0 \u00e0 9) et entrainer un mod\u00e8le en se basant sur les diff\u00e9rentes m\u00e9triques vu auparavant ne sera pas aussi simple. \n\nCet ensemble de donn\u00e9es servira \u00e9galement pour la transition vers l'apprentissage profond.\n\nImportons d'abord les librairies n\u00e9cessaires."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "import gzip\nimport struct\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, learning_curve, validation_curve\nfrom sklearn.datasets import load_digits\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_curve, classification_report, fbeta_score"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Et d\u00e9finissons l'ensemble de donn\u00e9es, t\u00e9l\u00e9chargeons le et chargeons le en m\u00e9moire."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "target_names = ['class 0', 'class 1', 'class 2', 'class 3', \n                'class 4', 'class 5', 'class 6', 'class 7', 'class 8', 'class 9']"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "def loadData(src, cimg):\n    with gzip.open(src) as gz:\n        n = struct.unpack('I', gz.read(4))\n        # Read magic number.\n        if n[0] != 0x3080000:\n            raise Exception('Invalid file: unexpected magic number.')\n        # Read number of entries.\n        n = struct.unpack('>I', gz.read(4))[0]\n        if n != cimg:\n            raise Exception('Invalid file: expected {0} entries.'.format(cimg))\n        crow = struct.unpack('>I', gz.read(4))[0]\n        ccol = struct.unpack('>I', gz.read(4))[0]\n        if crow != 28 or ccol != 28:\n            raise Exception('Invalid file: expected 28 rows/cols per image.')\n        # Read data.\n        res = np.frombuffer(gz.read(cimg * crow * ccol), dtype = np.uint8)\n    return res.reshape((cimg, crow * ccol))\n\ndef loadLabels(src, cimg):\n    with gzip.open(src) as gz:\n        n = struct.unpack('I', gz.read(4))\n        # Read magic number.\n        if n[0] != 0x1080000:\n            raise Exception('Invalid file: unexpected magic number.')\n        # Read number of entries.\n        n = struct.unpack('>I', gz.read(4))\n        if n[0] != cimg:\n            raise Exception('Invalid file: expected {0} rows.'.format(cimg))\n        # Read labels.\n        res = np.frombuffer(gz.read(cimg), dtype = np.uint8)\n    return res.reshape((cimg, 1))"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "import wget \n\n!rm './MNIST/raw/train-images-idx3-ubyte.gz' './MNIST/raw/train-labels-idx1-ubyte.gz' './raw/MNIST/t10k-images-idx3-ubyte.gz' './MNIST/raw/t10k-labels-idx1-ubyte.gz'\n\nwget.download('https://github.com/iid-ulaval/EEAA-datasets/raw/master/MNIST/train-images-idx3-ubyte.gz', './MNIST/raw/train-images-idx3-ubyte.gz')\nwget.download('https://github.com/iid-ulaval/EEAA-datasets/raw/master/MNIST/train-labels-idx1-ubyte.gz', './MNIST/raw/train-labels-idx1-ubyte.gz')\nwget.download('https://github.com/iid-ulaval/EEAA-datasets/raw/master/MNIST/t10k-images-idx3-ubyte.gz', './MNIST/raw/t10k-images-idx3-ubyte.gz')\nwget.download('https://github.com/iid-ulaval/EEAA-datasets/raw/master/MNIST/t10k-labels-idx1-ubyte.gz', './MNIST/raw/t10k-labels-idx1-ubyte.gz')\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "X_train = loadData('./MNIST/raw/train-images-idx3-ubyte.gz', 60000)\ny_train = loadLabels('./MNIST/raw/train-labels-idx1-ubyte.gz', 60000)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Vous pouvez regarder quelques exemples de donn\u00e9es avec le code ci-dessous : "}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "example_i = 5050\nplt.imshow(X_train[example_i,:].reshape(28,28), cmap=\"gray_r\")"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## D\u00e9coupage des donn\u00e9es de validation\n\nIci vous devez d\u00e9finir le d\u00e9coupage train/valid pour pouvoir s\u00e9lectionner votre mod\u00e8le. Ne touchez pas au test avant la fin de l'exercice ;-) \n\nVu qu'on est en multiclasse, n'oubliez pas d'utiliser la stratification de votre ensemble de donn\u00e9es pour garantir que toutes les classes sont bien repr\u00e9sent\u00e9es dans l'ensemble de validation (our cela utilisez le param\u00e8tre `stratify` de [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) en lui passant les \u00e9tiquettes `y`)."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# X_train, X_val, y_train, y_val = ..."}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## Courbe d'apprentissage\n\nChoisissez ici un classificateur ais\u00e9ment multiclasse (par exemple un arbre de d\u00e9cision : [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)) ou utilisez n'importe quel claissificateur binaire en *OneVsRest*. Le code apr\u00e8s va permettre de l'\u00e9valuer en validation crois\u00e9e selon son exactitude.\n\nNous allons \u00e9tudier \u00e0 quel point l'apport de nouveaux exemples aident l'apprentissage en favorisant la g\u00e9n\u00e9ralisation et quelle est la limite de la capacit\u00e9 d'un mod\u00e8le."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# from sklearn. ... import ... \n\n# estimator = ..."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "train_sizes=np.linspace(.1, 1.0, 5)\ntrain_sizes, train_scores, valid_scores = learning_curve(\n        estimator, X_train, y_train, cv=5, n_jobs=-1, train_sizes=train_sizes, scoring='accuracy', verbose=1)\n\n# Statistiques sur les diff\u00e9rents plis pour affichage ensuite\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\nvalid_scores_mean = np.mean(valid_scores, axis=1)\nvalid_scores_std = np.std(valid_scores, axis=1)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "plt.figure()\nplt.xlabel(\"Training examples\")\nplt.ylabel(\"Score\")\nplt.grid()\n\n# un ecart type sur les plis\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.3, color=\"r\")\nplt.fill_between(train_sizes, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.3, color=\"g\")\n\nplt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\nplt.plot(train_sizes, valid_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\nplt.legend(loc=\"best\")"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Cette figure pr\u00e9sente que les 15000 premiers exemples sont utiles, mais qu'en ajouter n'aide pas significativement l'apprentissage de par la limite \u00e9ventuelle de la capacit\u00e9 du mod\u00e8le ou le bruit al\u00e9atorique des donn\u00e9es."}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## Courbe de validation\n\nL\u00e0 o\u00f9 la courbe d'apprentissage mesure la performance en fonctiond du nombre d'exemples vus, la courbe de validation permet de mesurer la performance en fonction de la capacit\u00e9 du mod\u00e8le et des hyperparam\u00e8tres choisis. Dans l'exemple de code suivant (\u00e0 compl\u00e9ter selon le mod\u00e8le que vous avez choisi), il est possible de visualiser l'impach d'un seul hyperparam\u00e8tre.\n\nDans le cas de l'arbre de d\u00e9cision par exemple nous pourrions choisir l'hyperparam\u00e8tre `max_depth` et le faire varier de mani\u00e8re exponentielle entre 1 et 256 : `[1,2,4,8,16,32,64,128]`"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "hyper_name = 'max_depth'\nparam_range = [1,2,4,8,16,32,64,128,256]\n\n# Calcul de la courbe de validation avec validation crois\u00e9e\ntrain_scores, valid_scores = validation_curve(estimator, X_train, y_train, param_name=hyper_name, param_range=param_range, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1)\n\n# Calcul des statistiques pour visualisation\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\nvalid_scores_mean = np.mean(valid_scores, axis=1)\nvalid_scores_std = np.std(valid_scores, axis=1)\n\n# Visualisation\nplt.figure()\nplt.title(\"Validation Curve\")\nplt.xlabel(\"hyperparameter\")\nplt.ylabel(\"Score\")\nplt.ylim(0.0, 1.1)\nlw = 2\n\nplt.semilogx(param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", lw=lw)\nplt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.3, color=\"darkorange\", lw=lw)\nplt.semilogx(param_range, valid_scores_mean, label=\"Cross-validation score\", color=\"navy\", lw=lw)\nplt.fill_between(param_range, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.3, color=\"navy\", lw=lw)\nplt.legend(loc=\"best\")\nplt.show()"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## Entra\u00eenement d'un classificateur\n\nNous avons vu comment choisir le bon nombre de plis selon la courbe d'apprentissage et comment tester les hyperparam\u00e8tres un par un avec la courbe de validation. \u00c0 vous maintenant d'essayer d'entrainer le meilleur mod\u00e8le possible Scikit-Learn sur les donn\u00e9es MNIST \u00e0 l'aide de tous les outils dont vous disposez.\n\nUn exemple sommaire vous est donn\u00e9 ci-dessous.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(min_samples_split=10, n_jobs=-1, verbose=1)\nscore = cross_val_score(clf, X_train, y_train, cv=5, verbose=1, n_jobs=-1)\nnp.mean(score)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## \u00c9valuer le mod\u00e8le choisi sur le test\n\nUne fois que le classificateur est choisi avec le meilleur ensemble d'hyperparam\u00e8tres, il est temps de re\u00e9entrainer le mod\u00e8le choisi au complet sur la totalit\u00e9 des donn\u00e9es d'entrainement selon ces choix. Ce mod\u00e8le sera test\u00e9 sur l'ensemble de test. \n\nSi vous recommencez le processus apr\u00e8s cela parce que vous n'\u00eates pas satisfait du r\u00e9sultat sur l'ensemble de test, vous commencez \u00e0 *tricher* et \u00e0 surapprendre l'ensemble de test. La m\u00e9thodologie devient bancale, et vous perdez toute garantie de fonctionnement en production (toujours sous l'hypoth\u00e8se i.i.d  et que la distribution de g\u00e9n\u00e9ration des entr\u00e9es ne change pas). "}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "X_train = loadData('./MNIST/raw/train-images-idx3-ubyte.gz', 60000)\ny_train = loadLabels('./MNIST/raw/train-labels-idx1-ubyte.gz', 60000)\nX_test = loadData('./MNIST/raw/t10k-images-idx3-ubyte.gz', 10000)\ny_test = loadLabels('./MNIST/raw/t10k-labels-idx1-ubyte.gz', 10000)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "clf.fit(X_train, y_train)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "clf.score(X_test, y_test)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "N'h\u00e9sitez pas \u00e0 partager le r\u00e9sultat obtenu sur le forum en sp\u00e9cifiant le mod\u00e8le, les hyperparam\u00e8tres, et la graine de g\u00e9n\u00e9ration al\u00e9atoire choisie pour l'entrainement pour la r\u00e9plicabilit\u00e9 de vos r\u00e9sultats."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": ""}], "metadata": {"PAX": {"revision": 809, "userLang": "fr"}, "celltoolbar": "", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4-final"}, "revision": 809}, "nbformat": 4, "nbformat_minor": 2}