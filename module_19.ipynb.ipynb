{"cells": [{"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "QzyJqYieWPHN"}, "source": "---\n# Tutoriel 1 - Familiarisation avec MNIST et un premier Neural Net avec PyTorch.\n---\n\n<center><img src=\"https://python.gel.ulaval.ca/media/sio-u009/mlprocess_3.png\" alt=\"Processus d'apprentissage automatique\" width=\"50%\"/></center>\n\nDans ce tutoriel, nous allons reprendre l'exercice pr\u00e9c\u00e9dent sur les images de chiffres (MNIST) et reconstruire une r\u00e9seau de neurones simple en utilisant [PyTorch](https://pytorch.org/) la librairie d\u00e9velopp\u00e9e par Facebook pour faire des r\u00e9seaux de neurons plus complexes que les simples [MLPs de Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n\nVoici les diff\u00e9rentes \u00e9tapes suivies par ce tutoriel:\n\n1. Loading et exploration du dataset MNIST\n2. D\u00e9finition d'un r\u00e9seau simple\n3. D\u00e9finition de l'optimiseur\n4. D\u00e9finition de la fonction de perte et d'une m\u00e9trique\n5. Boucle d'entra\u00eenement\n    1. Piger une \"minibatch\" (pour la SGD)\n    2. Forward Pass\n    3. Back propagation\n    4. Optimisation\n6. R\u00e9sultats sur l'ensemble de test\n\nCommencons par charger les modules n\u00e9cessaires et d\u00e9finir quelques fonctions :"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "%matplotlib inline\nimport os\nimport math\nimport torch\nimport numpy as np\nfrom torch import optim, nn\nfrom torchvision.transforms import ToTensor\nfrom torchvision.datasets.mnist import MNIST\nfrom torch.utils.data import DataLoader, random_split\n\ntorch.manual_seed(42)\nnp.random.seed(42)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "HLhY4-bkksTd", "trusted": true}, "outputs": [], "source": "def load_mnist(download=False, path='./', transform=None):\n    \"\"\"Loads the MNIST dataset.\n\n    :param download: Download the dataset\n    :param path: Folder to put the dataset\n    :return: The train and test dataset\n    \"\"\"\n    train_dataset = MNIST(path, train=True, download=download, transform=transform)\n    test_dataset = MNIST(path, train=False, download=download, transform=transform)\n    return train_dataset, test_dataset\n\n\ndef load_mnist_with_validation_set(download=False, path='./', train_split=0.8):\n    \"\"\"Loads the MNIST dataset.\n\n    :param download: Download the dataset\n    :param path: Folder to put the dataset\n    :return: The train, valid and test dataset ready to be ingest in a neural network\n    \"\"\"\n    train, test = load_mnist(download, path, transform=ToTensor())\n    lengths = [round(train_split*len(train)), round((1.0-train_split)*len(train))]\n    train, valid = random_split(train, lengths)\n    return train, valid, test\n\ndef count_number_of_parameters(net):\n    \"\"\" Count the number of parameters of a neural net\n\n    :param net: a pytorch neural network\n    :return: The number of parameters in the net\n    \"\"\"\n    return sum(p.numel() for p in net.parameters() if p.requires_grad)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "tlhgw2brWPHS"}, "source": "## 1. Loading et exploration du dataset MNIST\n\nAnalysons plus en d\u00e9tails notre dataset, nos inputs et nos outputs (ou nos x et y)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 119}, "colab_type": "code", "deletable": false, "executionInfo": {"elapsed": 1719, "status": "ok", "timestamp": 1543538626105, "user": {"displayName": "Nicolas Garneau", "photoUrl": "", "userId": "14846125508805523822"}, "user_tz": 300}, "id": "AJwMKH8bWPHV", "outputId": "5f0673fe-e82d-4f0c-c05f-e816d7723909", "trusted": true}, "outputs": [], "source": "train, test = load_mnist(download=True)\n\n# Retirons les fichiers inutiles pour optimiser l'espace utilis\u00e9\n!rm -f ./MNIST/raw/*ubyte*"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 204}, "colab_type": "code", "deletable": false, "executionInfo": {"elapsed": 187, "status": "ok", "timestamp": 1543538627517, "user": {"displayName": "Nicolas Garneau", "photoUrl": "", "userId": "14846125508805523822"}, "user_tz": 300}, "id": "gXqFJXiGWPHZ", "outputId": "99dc2f77-b95f-4547-ddaa-3bb7db873840", "trusted": true}, "outputs": [], "source": "train, test"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "deletable": false, "executionInfo": {"elapsed": 234, "status": "ok", "timestamp": 1543538630189, "user": {"displayName": "Nicolas Garneau", "photoUrl": "", "userId": "14846125508805523822"}, "user_tz": 300}, "id": "cdpuybBGWPHd", "outputId": "15a2bfb6-b255-4b1a-93da-47448513a4ac", "trusted": true}, "outputs": [], "source": "len(train), len(test)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "deletable": false, "executionInfo": {"elapsed": 191, "status": "ok", "timestamp": 1543538631064, "user": {"displayName": "Nicolas Garneau", "photoUrl": "", "userId": "14846125508805523822"}, "user_tz": 300}, "id": "ig_Cv8H4WPHg", "outputId": "80f76ff3-00da-4424-f792-b7c28b67f333", "trusted": true}, "outputs": [], "source": "first_image = train[0]\nprint(first_image)\nfirst_image[0]"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "TvZMTuOcWPHo", "trusted": true}, "outputs": [], "source": "?ToTensor"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1921}, "colab_type": "code", "deletable": false, "executionInfo": {"elapsed": 286, "status": "ok", "timestamp": 1543538637187, "user": {"displayName": "Nicolas Garneau", "photoUrl": "", "userId": "14846125508805523822"}, "user_tz": 300}, "id": "TKwuwkLuWPHr", "outputId": "ee88fdbc-ca6f-4e38-fd04-ce7e69661ad2", "scrolled": true, "trusted": true}, "outputs": [], "source": "tensor = ToTensor()(first_image[0])\ntensor"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "deletable": false, "executionInfo": {"elapsed": 199, "status": "ok", "timestamp": 1543538641399, "user": {"displayName": "Nicolas Garneau", "photoUrl": "", "userId": "14846125508805523822"}, "user_tz": 300}, "id": "_u8uawNrWPHt", "outputId": "68468c18-82c9-4957-9e73-7f500a1dafbd", "trusted": true}, "outputs": [], "source": "tensor.shape"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "SJ4J64c4WPHy", "trusted": true}, "outputs": [], "source": "?train"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "PLYFx_ECWPH0", "trusted": true}, "outputs": [], "source": "train.transform = ToTensor()\ntest.transform = ToTensor()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "ND6uM2XrWPH2"}, "source": "### Cr\u00e9ation d'un ensemble de validation\n\nOn va prendre 20% des donn\u00e9es d'entrainement pour cela."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "FZJLSiWlWPH3", "trusted": true}, "outputs": [], "source": "lengths = [round(0.8*len(train)), round(0.2*len(train))]\ntrain, valid = random_split(train, lengths)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "deletable": false, "executionInfo": {"elapsed": 207, "status": "ok", "timestamp": 1543538647611, "user": {"displayName": "Nicolas Garneau", "photoUrl": "", "userId": "14846125508805523822"}, "user_tz": 300}, "id": "ijBzQAJrWPH6", "outputId": "91979bbb-5ecc-4743-e2d7-1b07a8e03654", "trusted": true}, "outputs": [], "source": "len(train), len(valid), len(test)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "cnyHxIQqWPH9"}, "source": "## 2. D\u00e9finition d'un r\u00e9seau simple"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Quelques configurations"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "use_gpu = torch.cuda.is_available()\nn_epoch = 20\nbatch_size = 32\nlearning_rate = 0.01"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Le tableau suivant est une repr\u00e9sentation du r\u00e9seau en couche. Le code ci-dessous utilise la mani\u00e8re orient\u00e9 objet de PyTorch d'impl\u00e9menter ce r\u00e9seau.\n\n| Type de couche              | Taille de sortie |      # de param\u00e8tres   |\n|-----------------------------|:----------------:|:----------------------:|\n| Input                       |   1x28x28   |              0            |\n| Flatten                     |  1\\*28\\*28  |              0            |\n| **Linear avec 10 neurones** |     10      | 28\\*28\\*10 + 10 = 7 850 |\n\n\\# total de param\u00e8tres du r\u00e9seau: 7 850\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 68}, "colab_type": "code", "deletable": false, "executionInfo": {"elapsed": 195, "status": "ok", "timestamp": 1543538651933, "user": {"displayName": "Nicolas Garneau", "photoUrl": "", "userId": "14846125508805523822"}, "user_tz": 300}, "id": "rEyOIU4ZWPH9", "outputId": "559cc80c-f0ea-4782-f289-03d5222c4292", "trusted": true}, "outputs": [], "source": "class Net(nn.Module): # La classe nn.Module repr\u00e9sente un r\u00e9seau de neurones dans PyTorch.\n    def __init__(self):\n        super(Net, self).__init__()\n        # Nous instancions les couches du r\u00e9seau dans la m\u00e9thode __init__.\n        self.fully_connected = nn.Linear(28 * 28, 10)\n\n    def forward(self, x):\n        # La m\u00e9thode forward effectue les calculs faits par le r\u00e9seau.\n        x = x.view(-1, 28 * 28)\n        x = self.fully_connected(x)\n        return x\n\nmodel = Net()\nif use_gpu:\n    model.cuda()\nmodel"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "deletable": false, "executionInfo": {"elapsed": 227, "status": "ok", "timestamp": 1543538660476, "user": {"displayName": "Nicolas Garneau", "photoUrl": "", "userId": "14846125508805523822"}, "user_tz": 300}, "id": "XObJbspuWPIC", "outputId": "659d4af8-ae38-4122-b947-5ca89dc549b4", "trusted": true}, "outputs": [], "source": "parameters = list(model.parameters())\nlen(parameters)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 204}, "colab_type": "code", "deletable": false, "executionInfo": {"elapsed": 273, "status": "ok", "timestamp": 1543538665098, "user": {"displayName": "Nicolas Garneau", "photoUrl": "", "userId": "14846125508805523822"}, "user_tz": 300}, "id": "PLga_0o6WPIG", "outputId": "079df2bf-7d93-4abf-9f97-a060734a6fa8", "trusted": true}, "outputs": [], "source": "parameters[0], parameters[1]"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "deletable": false, "executionInfo": {"elapsed": 181, "status": "ok", "timestamp": 1543538669773, "user": {"displayName": "Nicolas Garneau", "photoUrl": "", "userId": "14846125508805523822"}, "user_tz": 300}, "id": "GGLa7wLzWPIL", "outputId": "845b1489-5d56-4a2b-94cb-496770a6d612", "trusted": true}, "outputs": [], "source": "parameters[0].shape"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "deletable": false, "executionInfo": {"elapsed": 190, "status": "ok", "timestamp": 1543538673682, "user": {"displayName": "Nicolas Garneau", "photoUrl": "", "userId": "14846125508805523822"}, "user_tz": 300}, "id": "QvnF-RWJWPIR", "outputId": "50207e02-108d-483f-e434-6aa627760fa8", "trusted": true}, "outputs": [], "source": "count_number_of_parameters(model)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "euMMjHRoWPIT"}, "source": "## 3. D\u00e9finition de l'optimiseur\n\nL'optimiseur a besoin des param\u00e8tres du r\u00e9seau \u00e0 optimiser ainsi que du taux d'apprentissage (learning rate) pour conna\u00eetre la grandeur des pas \u00e0 faire pendant l'optimisation."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "WZfMwNRSWPIU", "trusted": true}, "outputs": [], "source": "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "mtrTOUvwWPIW"}, "source": "## 4. D\u00e9finition de la fonction de perte et d'une m\u00e9trique"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "VAR61UhIWPIX"}, "source": "NB: Dans PyTorch, la ```CrossEntropyLoss``` cache une ```Softmax```"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "bj7vmAP9WPIY", "trusted": true}, "outputs": [], "source": "criterion = nn.CrossEntropyLoss()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "w8h1hpH7WPIZ"}, "source": "Pour la m\u00e9trique, nous allons utiliser l'exactitude fournir par ```sklearn```"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "gVTyLS0fWPIb", "trusted": true}, "outputs": [], "source": "from sklearn.metrics import accuracy_score\ndef metric(y_true, y_pred): return accuracy_score(y_true, y_pred) * 100"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "WdRY3VFTWPId"}, "source": "## 5 Boucle d'entra\u00eenement"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Nous d\u00e9finissons des \"dataloaders\" qui vont nous donner `batch_size` exemples \u00e0 la fois, c'est-\u00e0-dire des batchs de taille `batch_size`. Nous avons 3 dataloaders pour les 3 ensembles utilis\u00e9s: entra\u00eenement, validation et test."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "6WwlWsEPWPIe", "trusted": true}, "outputs": [], "source": "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid, batch_size=batch_size)\ntest_loader = DataLoader(test, batch_size=batch_size)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Nous d\u00e9finissons maintenant une classe Trainer qui aura comme t\u00e2che d'effectuer l'entra\u00eenement et l'\u00e9valuation de notre mod\u00e8le. Dans le prochain tutoriel, nous verrons la librairie Poutyne qui va faire tout \u00e7a pour nous et bien plus.\n\nUn des d\u00e9tail \u00e0 comprendre \u00e0 lisant le code ci-dessous est que le gradient est stock\u00e9 dans les param\u00e8tres lorsque PyTorch le calcule pour nous. Nous n'avons donc pas besoin d'acc\u00e9der au gradient directement. L'optimiseur se charge de tout pour nous."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "fH8VnHROWPIh", "trusted": true}, "outputs": [], "source": "class Trainer:\n    def __init__(self, model, optimizer, criterion, metric, use_gpu=False):\n        self.model = model\n        self.optimizer = optimizer\n        self.criterion = criterion\n        self.metric = metric\n        self.use_gpu = use_gpu\n        \n    def evaluate_model(self, loader):\n        true = []\n        pred = []\n        val_loss = []\n\n        # Activation du mode d'\u00e9valuation du mod\u00e8le\n        self.model.eval()\n        \n        # D\u00e9sactivation du calcul du gradient en \u00e9valuation\n        with torch.no_grad():\n            for batch in loader:\n                inputs, targets = batch\n                \n                # Envoie de la batch sur GPUs le cas \u00e9ch\u00e9ant\n                if self.use_gpu:\n                    inputs = inputs.cuda()\n                    targets = targets.cuda()\n\n                # Ceci appelle ma m\u00e9thode forward de notre mod\u00e8le.\n                output = self.model(inputs)\n\n                predictions = output.max(dim=1)[1]\n\n                val_loss.append(self.criterion(output, targets).item())\n                true.extend(targets.detach().cpu().numpy().tolist())\n                pred.extend(predictions.detach().cpu().numpy().tolist())\n\n        return self.metric(true, pred), sum(val_loss) / len(val_loss)\n\n    def train_model(self, train_loader, valid_loader, n_epoch):\n        for i in range(n_epoch):\n            # Activation du mode d'entra\u00eenement du mod\u00e8le\n            self.model.train()\n            \n            for batch in train_loader:\n                inputs, targets = batch\n                \n                # Envoie de la batch sur GPUs le cas \u00e9ch\u00e9ant\n                if self.use_gpu:\n                    inputs = inputs.cuda()\n                    targets = targets.cuda()\n                \n                # Le gradient est mis \u00e0 z\u00e9ro pour \u00e9viter que le gradient de la batch\n                # pr\u00e9c\u00e9dente soit m\u00e9lang\u00e9 au gradient de la nouvelle batch.\n                self.optimizer.zero_grad()\n                \n                # Ceci appelle ma m\u00e9thode forward de notre mod\u00e8le.\n                output = self.model(inputs)\n\n                # Calcul de la perte\n                loss = self.criterion(output, targets)\n                \n                # Retropropagation du gradient (le gradient est stock\u00e9 dans les param\u00e8tres)\n                loss.backward()\n                \n                # Mise \u00e0 jour des poids en fonction du gradient calcul\u00e9.\n                self.optimizer.step()\n\n            train_acc, train_loss = self.evaluate_model(train_loader)\n            val_acc, val_loss = self.evaluate_model(valid_loader)\n            \n            print('Epoch {} - Train acc: {:.2f} - Val acc: {:.2f} - Train loss: {:.4f} - Val loss: {:.4f}'.format(\n                i,\n                train_acc,\n                val_acc,\n                train_loss,\n                val_loss\n            ))"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "C'est maintenant le temps d'entra\u00eener et de tester notre mod\u00e8le."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 394}, "colab_type": "code", "deletable": false, "executionInfo": {"elapsed": 283267, "status": "ok", "timestamp": 1543538984044, "user": {"displayName": "Nicolas Garneau", "photoUrl": "", "userId": "14846125508805523822"}, "user_tz": 300}, "id": "KZ3AVGJbWPIj", "outputId": "364b4c85-ac02-4057-e965-bc0d02812485", "trusted": true}, "outputs": [], "source": "trainer = Trainer(model, optimizer, criterion, metric, use_gpu)\ntrainer.train_model(train_loader, valid_loader, n_epoch)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 54}, "colab_type": "code", "deletable": false, "executionInfo": {"elapsed": 1456, "status": "ok", "timestamp": 1543539006016, "user": {"displayName": "Nicolas Garneau", "photoUrl": "", "userId": "14846125508805523822"}, "user_tz": 300}, "id": "SRTFRsxuWPIl", "outputId": "d2466c39-4639-4099-af5a-c8265ced9c43", "trusted": true}, "outputs": [], "source": "test_acc, test_loss = trainer.evaluate_model(test_loader)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "deletable": false, "executionInfo": {"elapsed": 205, "status": "ok", "timestamp": 1543539007467, "user": {"displayName": "Nicolas Garneau", "photoUrl": "", "userId": "14846125508805523822"}, "user_tz": 300}, "id": "UzEBGGGjWPIo", "outputId": "79cb4268-51e0-4d50-bf28-bcd3e4e664ab", "trusted": true}, "outputs": [], "source": "test_acc, test_loss"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": ""}], "metadata": {"PAX": {"revision": 812, "userLang": "fr"}, "accelerator": "GPU", "celltoolbar": "", "colab": {"collapsed_sections": [], "name": "Tutoriel 1.ipynb", "provenance": [], "version": "0.3.2"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.1"}, "nbTranslate": {"displayLangs": ["*"], "hotkey": "alt-t", "langInMainMenu": true, "sourceLang": "en", "targetLang": "fr", "useGoogleTranslate": true}, "revision": 812}, "nbformat": 4, "nbformat_minor": 1}