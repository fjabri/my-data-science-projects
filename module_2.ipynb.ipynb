{"cells": [{"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "CDEWTP-vNB_R"}, "source": "---\n# Pr\u00e9paration des Donn\u00e9es\n\n---\n\n\nPour pr\u00e9parer ad\u00e9quatement les donn\u00e9es avant de les fournir \u00e0 un ou plusieurs algorithmes d'apprentissage automatique, il faut \u00e9galement s'assurer de l'int\u00e9grit\u00e9 de ces donn\u00e9es pour ne pas fournir de valeurs manquantes aux algorithmes qui pourraient alors donner une valeur manquante \u00e0 leur tour.\n\n<center><img src=\"https://python.gel.ulaval.ca/media/sio-u009/mlprocess_2.png\" alt=\"Processus d'apprentissage automatique\" width=\"50%\"/></center>\n\n1. Le nettoyage et les aberrations statistiques\n2. **L'imputation de donn\u00e9es manquantes**\n3. \u00c9quilibrage de donn\u00e9es d\u00e9s\u00e9quilibr\u00e9es\n4. Transformation des caract\u00e9ristiques\n    1. *rescaling* et *normalizing* (\\[0, 1\\] ou \\[-1, 1\\]), *standardizing* (loi normale)\n    2. Repr\u00e9sentation matricielle de donn\u00e9es cat\u00e9goris\u00e9es\n    3. R\u00e9duction de la dimensionnalit\u00e9 ou cr\u00e9ation de caract\u00e9ristiques\n\n\n\n"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "0yQg0QdRJYFn"}, "source": "## 2. Imputation de donn\u00e9es manquantes \nSi vous voulez faire simple : `scikit-learn` offre un [`SimpleImputer`](http://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) et un [`MissingIndicator`](http://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator) alors que `pandas` offre la m\u00e9thode [`fillna()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html).\n\nSinon : Fonctionalit\u00e9s de la librairie [`impypute`](https://pypi.org/project/impyute/):\n- Outils de diagnostic\n     - Journaux\n     - Distribution des valeurs nulles\n     - Comparaison des imputations\n     - [Test MCAR de Little [1]](#note1)\n- Imputation de donn\u00e9es transversales\n     - Imputation al\u00e9atoire\n     - K-voisins les plus proches\n     - Imputation moyenne\n     - Imputation par mode\n     - Imputation m\u00e9diane\n     - Imputation multivari\u00e9e par \u00e9quations cha\u00een\u00e9es ([MICE](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/))\n     - Esp\u00e9rance/Maximisation\n- Imputation de donn\u00e9es chronologiques\n     - Derni\u00e8re observation report\u00e9e\n     - Fen\u00eatre mobile\n     - Moyenne mobile int\u00e9gr\u00e9e autor\u00e9gressive (WIP)\n     \nOu bien : [`fancyimpute`](https://pypi.org/project/fancyimpute/) :\n* `SimpleFill`: Remplace les entr\u00e9es manquantes par la moyenne ou la m\u00e9diane de chaque colonne.\n* `KNN`: imputations du voisin le plus proche qui pond\u00e8re les \u00e9chantillons en utilisant la diff\u00e9rence quadratique moyenne sur les entit\u00e9s pour lesquelles deux lignes contiennent des donn\u00e9es observ\u00e9es.\n* `SoftImpute`: compl\u00e9tion de la matrice par seuillage souple it\u00e9ratif des d\u00e9compositions SVD. Inspir\u00e9 du package [softImpute](https://web.stanford.edu/~hastie/swData/softImpute/vignette.html) pour R, bas\u00e9 sur [Spectral Regularization Algorithms for Learning Large Incomplete Matrices](http://web.stanford.edu/~hastie/Papers/mazumder10a.pdf) de Mazumder et. Al.\n* `IterativeSVD`: ach\u00e8vement de la matrice par d\u00e9composition it\u00e9rative SVD de bas rang. Devrait \u00eatre similaire \u00e0 SVDimpute de [Missing value estimation methods for DNA microarrays](http://www.ncbi.nlm.nih.gov/pubmed/11395428) de Troyanskaya et. Al.\n* `IterativeImputer` (ex MICE): Une strat\u00e9gie pour imputer les valeurs manquantes en mod\u00e9lisant chaque entit\u00e9 avec des valeurs manquantes en fonction des autres entit\u00e9s de mani\u00e8re altern\u00e9e.\n* `MatrixFactorization`: Factorisation directe de la matrice incompl\u00e8te en\u00ab U \u00bbet\u00ab V \u00bbde bas rang, avec une p\u00e9nalit\u00e9 de faible densit\u00e9 L1 sur les \u00e9l\u00e9ments de\u00ab U \u00bbet une p\u00e9nalit\u00e9 de L2 sur les \u00e9l\u00e9ments de\u00ab V \u00bb. R\u00e9solu par descente progressive.\n* `NuclearNormMinimization`: Impl\u00e9mentation simple de [Exact Matrix Completion via Convex Optimization](http://statweb.stanford.edu/~candes/papers/MatrixCompletion.pdf) d'Emmanuel Candes et Benjamin Recht utilisant [cvxpy](http://www.cvxpy.org). Trop lent pour les grandes matrices.\n* `BiScaler`: Estimation it\u00e9rative de la moyenne des rang\u00e9es/colonnes et des \u00e9carts types pour obtenir une double normalisationmatrice. Pas garanti de converger mais fonctionne bien dans la pratique. Tir\u00e9 de [Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares](http://arxiv.org/abs/1410.2596)."}, {"cell_type": "markdown", "metadata": {"_cell_guid": "b57ada49-2e30-473d-9e6e-fa57afd87bc5", "_uuid": "c66c9a187266122b4fdc5b50ad650988f9e23603", "colab_type": "text", "editable": false, "id": "aborTJDPJYFn"}, "source": "Essayons de manipuler des donn\u00e9es manquantes de la comp\u00e9tition Kaggle de titanic[$^1$](https://www.kaggle.com/c/titanic):\n\nNous aurons \u00e0 retravailler dessus lors des exercices de classification (Module 3)\n\n### Chargement des donn\u00e9es\n\n - survival: Survival (0 = No, 1 = Yes)\n - pclass: Ticket class(1 = 1st, 2 = 2nd, 3 = 3rd)\n - sex: Sex\n - Age: Age in years\n - sibsp: Number of siblings / spouses aboard the Titanic\n - parch: Number of parents / children aboard the Titanic\n - ticket: Ticket number\n - name: Passenger name\n - fare: Passenger fare\n - embarked: Port of Embarkation(C = Cherbourg, Q = Queenstown, S = Southampton)\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "emJ03L5ZJYFo", "trusted": true}, "outputs": [], "source": "import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport wget\n\n!rm './titanic_train.csv' './titanic_test.csv'\nwget.download('https://raw.githubusercontent.com/iid-ulaval/EEAA-datasets/master/titanic_train.csv','./titanic_train.csv')\nwget.download('https://raw.githubusercontent.com/iid-ulaval/EEAA-datasets/master/titanic_test.csv','./titanic_test.csv')\n\n\nnp.random.seed(0)\n\ntrain = pd.read_csv('titanic_train.csv')\ntest = pd.read_csv('titanic_test.csv')"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "j_saXmmeJYFo"}, "source": "Cr\u00e9ation des types de colonnes\n\n(On vous laisse deviner ce que ca fait, mais posez la question sur le forum si jamais ce n'est pas clair)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "vWnRnySaJYFp", "trusted": true}, "outputs": [], "source": "import sys\nimport warnings\n\ndef enforceTypesTitanic(df):\n    Pclass_dtype = pd.api.types.CategoricalDtype(categories=[1, 2, 3], ordered=True)\n    df.Survived = df.Survived.astype(\"category\")\n    df.Pclass = df.Pclass.astype(Pclass_dtype)\n    df.Sex = df.Sex.astype(\"category\")\n    df.Embarked = df.Embarked.astype(\"category\")\n    df = df.drop('Name', axis=1)\n    df = df.drop('Ticket', axis=1)\n    df = df.drop('index', axis=1)\n    return df\n\n\ntrain = enforceTypesTitanic(train)\ntest = enforceTypesTitanic(test)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "HuJh9nAPJYFr"}, "source": "Comptage des valeurs nulles dans le dataset (`train` et `test`):"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "Eis5UCqTJYFr", "trusted": true}, "outputs": [], "source": "def naSummary(df):\n    return df.isnull().sum()\n\nnaSummary(train)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "-jsT7Jr3JYFv", "trusted": true}, "outputs": [], "source": "naSummary(test)"}, {"cell_type": "markdown", "metadata": {"_cell_guid": "602b7065-1c54-4a77-8e47-4836e68a0a05", "_uuid": "008a9e79ab99b1553bd002d0b745a85fa11f08a2", "colab_type": "text", "editable": false, "id": "1e4LNHFNJYFw"}, "source": "## Validation de l'\u00e9quilibre de la distribution des donn\u00e9es Train vs Test"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "ABTHqlrMJYFx", "trusted": true}, "outputs": [], "source": "%matplotlib inline\nimport seaborn as sns\nimport matplotlib.gridspec as gs\nimport matplotlib.pyplot as plt\nimport itertools\n\n# Si vous avez statsmodels install\u00e9, \nsns.distributions._has_statsmodels=False\n\n# Calculer la taille de la grille en fonction du nombre de caract\u00e9ristiques \ndef gridSize(nb_features):\n    a = len(nb_features)\n    if a%2 != 0:\n        a += 1\n    n = np.floor(np.sqrt(a)).astype(np.int64)\n    while a%n != 0:\n        n -= 1\n    m = (a/n).astype(np.int64)\n    return m,n\n\n# Affichage des deux distributions pour chaque colonne des dataframe 1 et 2\ndef distComparison(df1, df2):\n    \n    assert (len(df1.columns) == len(df2.columns))\n    \n    m,n = gridSize(df1.columns)\n    coords = list(itertools.product(list(range(m)), list(range(n))))\n    \n    # Choix des graphiques pour chaque type de colonne\n    numerics = df1.select_dtypes(include=[np.number]).columns\n    cats = df1.select_dtypes(include=['category']).columns\n    \n    fig = plt.figure(figsize=(15, 15))\n    axes = gs.GridSpec(m, n)\n    axes.update(wspace=0.25, hspace=0.25)\n    # Graphiques pour donn\u00e9es num\u00e9riques : on fait un KDE de la distribution\n    for i in range(len(numerics)):\n        x, y = coords[i]\n        ax = plt.subplot(axes[x, y])\n        col = numerics[i]\n\n        sns.kdeplot(df1[col].dropna(), ax=ax, label='df1').set(xlabel=col)\n        sns.kdeplot(df2[col].dropna(), ax=ax, label='df2')\n        \n    # Graphique pour les donn\u00e9es cat\u00e9goriques : diagramme en baton\n    for i in range(0, len(cats)):\n        x, y = coords[len(numerics)+i]\n        ax = plt.subplot(axes[x, y])\n        col = cats[i]\n\n        df1_temp = df1[col].value_counts()\n        df2_temp = df2[col].value_counts()\n        df1_temp = pd.DataFrame({col: df1_temp.index, 'value': df1_temp/len(df1), 'Set': np.repeat('df1', len(df1_temp))})\n        df2_temp = pd.DataFrame({col: df2_temp.index, 'value': df2_temp/len(df2), 'Set': np.repeat('df2', len(df2_temp))})\n\n        sns.barplot(x=col, y='value', hue='Set', data=pd.concat([df1_temp, df2_temp]), ax=ax).set(ylabel='Percentage')\n\n# Affichage de la comparaison entre train (sans la colonne des \u00e9tiquettes) et le test\ndistComparison(train, test)"}, {"cell_type": "markdown", "metadata": {"_cell_guid": "eb108455-8d35-42b5-a6e6-0d644d30959e", "_uuid": "de2217f045c02397cf908b43a38ee9dc9b86fc6b", "colab_type": "text", "editable": false, "id": "C2LpW0gyJYFy"}, "source": "On se cr\u00e9e une baseline en compl\u00e9tant les donn\u00e9es arbitrairement."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "VihMo6p4JYFy", "trusted": true}, "outputs": [], "source": "# Embarked est le port d'embaquement, on compl\u00e8te les donn\u00e9es manquantes par le premier port.\ntrain.Embarked = train.Embarked.fillna('C')\n\n# Et il nous manque une donn\u00e9e de prix pay\u00e9. On choisis une valeur arbitraire.\ntrain.Fare = train.Fare.fillna(8.05)\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "GkAKK2JaJYFz", "trusted": true}, "outputs": [], "source": "naSummary(train)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "9P5_82N2JYF1", "trusted": true}, "outputs": [], "source": "naSummary(test)"}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d36d00c2-6fd3-4b5f-9140-3dff423e0b87", "_uuid": "f4046a90e27ad0913eca82b47cc76c2d07850ccd", "colab_type": "text", "editable": false, "id": "4tL9dlDwJYF1"}, "source": "## Les donn\u00e9es sont elles MCAR ?\n\nMCAR = missing completely at random.\n\nEssentiellement, divisons les donn\u00e9es en deux ensembles suppl\u00e9mentaires: Donn\u00e9es manquantes et donn\u00e9es pr\u00e9sentes. Puis v\u00e9rifions que, si la distribution des variables dans chacun de ces ensembles est la m\u00eame, alors les donn\u00e9es manquent compl\u00e8tement au hasard."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "_lb3qwxGJYF1", "trusted": true}, "outputs": [], "source": "age_present = train.dropna().drop('Age', 1)\nage_missing = train[train.isnull().any(axis=1)].drop('Age', 1)\n\ncat_dtype = pd.api.types.CategoricalDtype(categories=list(range(8)), ordered=True)\nage_present.Parch = age_present.Parch.astype(cat_dtype)\nage_missing.Parch = age_missing.Parch.astype(cat_dtype)\n\nage_present.SibSp = age_present.SibSp.astype(cat_dtype)\nage_missing.SibSp = age_missing.SibSp.astype(cat_dtype)\n\ndistComparison(age_present.drop('Survived', 1), age_missing.drop('Survived', 1))"}, {"cell_type": "markdown", "metadata": {"_cell_guid": "ec01755d-01b8-45e8-b57a-7f83ff302927", "_uuid": "0fee94f930b9a76b642e4182f2e0a073da427c6c", "colab_type": "text", "editable": false, "id": "IqyeRSDqJYF3"}, "source": "Il semble que nous ne puissions pas v\u00e9rifier l\u2019hypoth\u00e8se MCAR. L'explication semble \u00eatre que nous sommes moins susceptibles de conna\u00eetre l'\u00e2ge des personnes d\u00e9c\u00e9d\u00e9es. Comme en t\u00e9moigne la proportion beaucoup plus grande de passagers de la classe inf\u00e9rieure, le pic plus net dans les tarifs plus bas et une l\u00e9g\u00e8re asym\u00e9trie \u00e0 l\u2019\u00e9gard des hommes.\n\nDe mani\u00e8re plus significative encore, il semble que les personnes qui se sont embarqu\u00e9es \u00e0 Q ont un taux beaucoup plus \u00e9lev\u00e9 d\u2019\u00e2ge manquant.\n\nRemarque: Il serait pr\u00e9f\u00e9rable d'utiliser une mesure plus objective de MCAR, comme le test de Little : (TODO)"}, {"cell_type": "markdown", "metadata": {"_cell_guid": "8be28abb-f498-450a-9d25-4771c432ecec", "_uuid": "1eb821dba55457db166b63b8bf943bb1f5a30164", "colab_type": "text", "editable": false, "id": "FhCu5EkbJYF3"}, "source": "## Baseline de pr\u00e9diction"}, {"cell_type": "markdown", "metadata": {"_cell_guid": "64668e8b-0ae7-4c4e-8062-afdaf708c9d4", "_uuid": "5079fff2dea16b57d9c220e40b694a4dde489b38", "colab_type": "text", "editable": false, "id": "CogyYiFvJYF3"}, "source": "Sans l'utilisation des donn\u00e9es `Age`, notre pr\u00e9dicteur sera un classificateur par for\u00eat al\u00e9atoire avec les param\u00e8tres affich\u00e9s. Toutes les estimations d'erreur de test sont obtenues par une validation crois\u00e9e 10 fois."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "QyL2gc08JYF3", "trusted": true}, "outputs": [], "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Pr\u00e9paration des donn\u00e9es rapides pour enlever les donn\u00e9es cat\u00e9goriques\ndef prepForModel(df):\n    new_df = df.copy()\n    # Classe de cabine en entier\n    new_df.Pclass = new_df.Pclass.astype(\"int\")\n    # Sexe binaire\n    new_df.Sex.cat.categories = [0, 1]\n    new_df.Sex = new_df.Sex.astype(\"int\")\n    # Port d'embarquement en entier (on pourrait utiliser une dummy ? tryit ;-))\n    new_df.Embarked.cat.categories = [0, 1, 2]\n    new_df.Embarked = new_df.Embarked.astype(\"int\")\n    return new_df\n\n# M\u00eame pipeline pour le train et le test.\ntrain_cl = prepForModel(train)\ntest_cl = prepForModel(test)\n\n# S\u00e9lection des colonnes sans l'Age\nXcol = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']\nYcol = 'Survived'\nX = train_cl[Xcol]\nY = train_cl[Ycol]\n\n# M\u00e9morisation des datasets avant qu'on fasse d'autres transformations\nXbase = X\nYbase = Y\n\n# Classification par RF\nrf = RandomForestClassifier(n_estimators=1000,\n                           max_depth=None,\n                           min_samples_split=10)\n\nbaseline_err = cross_val_score(rf, X, Y, cv=10, n_jobs=-1).mean()\nprint(\"[BASELINE] Estimation RF sur Test (n = {}, 10-fold CV): {}\".format(len(X), baseline_err))"}, {"cell_type": "markdown", "metadata": {"_cell_guid": "9d2a87fb-4317-4fd2-a6db-f5bd62fb3362", "_uuid": "bcad8d2354eb04f7139567624d27ac2d9e73ad5a", "colab_type": "text", "editable": false, "id": "1IKAK8N8JYF4"}, "source": "#### Suppression simple des donn\u00e9es manquantes "}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "GgZwr3c2JYF5", "trusted": true}, "outputs": [], "source": "Xdel = train_cl.dropna()[Xcol + ['Age']]\nYdel = train_cl.dropna()[Ycol]\n\ndeletion_err = cross_val_score(rf, Xdel, Ydel, cv=10, n_jobs=-1).mean()\nprint(\"[DELETION] Estimation RF sur Test (n = {}, 10-fold CV): {}\".format(len(Xdel), deletion_err))"}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a6ac5c2b-364b-476d-8786-dd9f0fd7a9cc", "_uuid": "4dc1d8e53ddc961c92a334367201c89cdc21af3b", "colab_type": "text", "editable": false, "id": "zpKzjUUTJYGD"}, "source": "#### Substitution par la moyenne"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "rF99L16KJYGD", "trusted": true}, "outputs": [], "source": "train_cl = prepForModel(train)\ntrain_cl.Age = train_cl.Age.fillna(train_cl.Age.mean(skipna=True))\n\nXmean = train_cl[Xcol + ['Age']]\nYmean = train_cl[Ycol]\n\nmean_err = cross_val_score(rf, Xmean, Ymean, cv=10, n_jobs=-1).mean()\nprint(\"[MEAN] Estimation RF sur Test (n = {}, 10-fold CV): {}\".format(len(Xmean), mean_err))"}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a52e28e5-1333-4c96-bda7-10b36c39c137", "_uuid": "ec26ebd70984436e825d406d2616830f0f12e9cc", "colab_type": "text", "editable": false, "id": "XyQfkRuAJYGE"}, "source": "#### R\u00e9gression d\u00e9terministe et al\u00e9atoire"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "DOu1H8vAJYGE", "trusted": true}, "outputs": [], "source": "train_cl = prepForModel(train)\ntrain_reg = train_cl.dropna()\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import preprocessing\n\nXrcol = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']\nYrcol = 'Age'\n\nX_reg = train_reg[Xrcol]\nY_reg = train_reg[Yrcol]\n\nage_lm = LinearRegression()\nage_lm.fit(X_reg, Y_reg)\nabs_residuals = np.absolute(Y_reg - age_lm.predict(X_reg))\n\nnan_inds = train_cl.Age.isnull().to_numpy().nonzero()[0]\ntrain_cl2 = train_cl.copy()\n\nfor i in nan_inds:\n    train_cl['Age'].at[i] = age_lm.predict(train_cl.loc[i, Xrcol].values.reshape(1, -1))\n\nXreg = train_cl[Xcol + ['Age']]\nYreg = train_cl[Ycol]\n    \nreg_err = cross_val_score(rf, Xreg, Yreg, cv=10, n_jobs=-1).mean()\nprint(\"[DETERMINISTIC REGRESSION] Estimation RF sur Test (n = {}, 10-fold CV): {}\".format(len(Xreg), reg_err))\n\nfor i in nan_inds:\n    detreg = age_lm.predict(train_cl2.loc[i, Xrcol].values.reshape(1, -1))\n    randreg = np.random.normal(detreg, np.random.choice(abs_residuals))\n    train_cl2['Age'].at[i] = randreg\n    \nXrandreg = train_cl2[Xcol + ['Age']]\nYrandreg = train_cl2[Ycol]\n    \nrandreg_err = cross_val_score(rf, Xrandreg, Yrandreg, cv=10, n_jobs=-1).mean()\nprint(\"[RANDOM REGRESSION] Estimation RF sur Test (n = {}, 10-fold CV): {}\".format(len(Xrandreg), randreg_err))"}, {"cell_type": "markdown", "metadata": {"_cell_guid": "30dc53c5-cb32-4d44-8d23-d2c806a9cd23", "_uuid": "408dbaa3bdcefb881222e942070e050e36bd40f3", "colab_type": "text", "editable": false, "id": "iYhXrIOHJYGG"}, "source": "#### MICE"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "eofzVHGiJYGG", "trusted": true}, "outputs": [], "source": "#!pip install fancyimpute \n# En cas de probl\u00e8me avec cvxpy sur windows : http://www.lfd.uci.edu/~gohlke/pythonlibs/\nfrom fancyimpute import IterativeImputer # MICE a \u00e9t\u00e9 renomm\u00e9 au mois de Septembre ...\n\ntrain_cl = prepForModel(train)\n\nX = train_cl.loc[:, Xcol + ['Age']]\nY = train_cl.loc[:, Ycol]\n\nXmice = IterativeImputer().fit_transform(X)\nYmice = Y\n\nmice_err = cross_val_score(rf, Xmice, Y, cv=10, n_jobs=-1).mean()\nprint(\"[MICE] Estimation RF sur Test (n = {}, 10-fold CV): {}\".format(len(Xmice), mice_err))"}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d37c7551-67bd-4e7f-94d9-25a950021876", "_uuid": "5cb98a3c43ea86f8348b529623eb2cc39f3970c3", "colab_type": "text", "editable": false, "id": "ktNAQdMuJYGG"}, "source": "#### KNN (Donn\u00e9es standardis\u00e9es)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "GEMMkK4mJYGH", "trusted": true}, "outputs": [], "source": "from fancyimpute import KNN\nfrom sklearn.model_selection import StratifiedKFold\n\ntrain_cl = prepForModel(train)\n\nXcol = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']\nX = train_cl.loc[:, Xcol + ['Age']]\nY = train_cl.loc[:, Ycol]\n\ndef standardize(s):\n    return s.sub(s.min()).div((s.max() - s.min()))\n\nXnorm = X.apply(standardize, axis=0)\nkvals = np.linspace(1, 100, 20, dtype='int64')\n\nknn_errs = []\nfor k in kvals:\n    knn_err = []\n    Xknn = KNN(k=k, verbose=False).fit_transform(Xnorm)\n    knn_err = cross_val_score(rf, Xknn, Y, cv=10, n_jobs=-1).mean()\n\n    knn_errs.append(knn_err)\n    print(\"[KNN] Estimation RF sur Test (n = {}, k = {}, 10-fold CV): {}\".format(len(Xknn), k, np.mean(knn_err)))"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "jEewLi06JYGI", "trusted": true}, "outputs": [], "source": "sns.set_style(\"darkgrid\")\n_ = plt.plot(kvals, knn_errs)\n_ = plt.xlabel('K')\n_ = plt.ylabel('10-fold CV Error Rate')\n\nknn_err = max(knn_errs)\nk_opt = kvals[knn_errs.index(knn_err)]\n\nXknn = KNN(k=k_opt, verbose=False).fit_transform(Xnorm)\nYknn = Y\n\nprint(\"[BEST KNN] Estimation RF sur Test (n = {}, k = {}, 10-fold CV): {}\".format(len(Xknn), k_opt, np.mean(knn_err)))"}, {"cell_type": "markdown", "metadata": {"_cell_guid": "b6b05aaf-e846-4225-95c0-e66f807e61ea", "_uuid": "c02e770675a39fd1b4c4e3f13639b852f5a24d7d", "colab_type": "text", "editable": false, "id": "Lwhqq6gBJYGJ"}, "source": "#### R\u00e9sum\u00e9 : "}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "csxBlx8eJYGK", "trusted": true}, "outputs": [], "source": "errs = {'BEST KNN (k = {})'.format(k_opt): knn_err,  \n        'DETERMINISTIC REGRESSION': reg_err, \n        'RANDOM REGRESSION': randreg_err,\n        'MICE': mice_err,\n        'MEAN': mean_err,\n        'DELETION': deletion_err, \n        'BASELINE': baseline_err}\n\nerr_df = pd.DataFrame.from_dict(errs, orient='index')\nerr_df.index.name = 'Imputation Method'\nerr_df.reset_index(inplace=True)\nerr_df.columns = ['Imputation', ' Estimation sur Test  (10-fold CV)']\n\nax = sns.barplot(x=err_df.columns[1], y=err_df.columns[0], order=list.sort(list(errs.values())), data=err_df)\nax.set_xlabel(err_df.columns[1])\nax.set_ylabel('')\n_ = plt.xlim(0.7, 0.8)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "### Que faut il tirer de ce graphique ?\n\nQue si on compl\u00e8te les donn\u00e9es manquantes avec divers mod\u00e8les on am\u00e9liore le pouvoir de pr\u00e9diction ... an ajoutant un biais dans les donn\u00e9es :-)"}], "metadata": {"PAX": {"revision": 793, "userLang": "fr"}, "celltoolbar": "", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.1"}, "revision": 793, "varInspector": {"cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"delete_cmd_postfix": "", "delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())"}, "r": {"delete_cmd_postfix": ") ", "delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": false}}, "nbformat": 4, "nbformat_minor": 1}