{"cells": [{"cell_type": "markdown", "metadata": {"editable": false}, "source": "---\n# Tutoriel 4 - Transfert d'apprentissage\n---\n\n<center><img src=\"https://python.gel.ulaval.ca/media/sio-u009/mlprocess_3.png\" alt=\"Processus d'apprentissage automatique\" width=\"50%\"/></center>\n\nDans ce tutoriel, nous allons effectuer ce qu'on appelle le transfert d'apprentissage. Pour ce faire, nous allons utiliser un r\u00e9seau \u00e0 convolution de type [ResNet](https://en.wikipedia.org/wiki/Residual_neural_network) qui a \u00e9t\u00e9 pr\u00e9-entra\u00een\u00e9 avec le jeu de donn\u00e9es [ImageNet](https://fr.wikipedia.org/wiki/ImageNet). \u00c9tant donn\u00e9 que les donn\u00e9es de ImageNet sont assez semblables \u00e0 ceux de CIFAR10, le r\u00e9seau ResNet pr\u00e9-entra\u00een\u00e9 est capable d'extraire des features (attributs) utiles \u00e0 la d\u00e9tection d'objet. On peut donc s'attendre \u00e0 de meilleurs performances sur CIFAR10. \n\nPour effectuer le transfert, la m\u00e9canique est la suivante. Les poids pr\u00e9-entra\u00een\u00e9s du ResNet sont charg\u00e9s dans notre mod\u00e8le PyTorch. \u00c9tant donn\u00e9 que ImageNet a 1000 classes et que CIFAR10 en a seulement 10, la couche du ResNet qui fait la classification (aussi nomm\u00e9 t\u00eate du r\u00e9seau) est chang\u00e9 pour avoir seuelement 10 sorties au lieu de 1000. Une fois cela fait, nous pouvons entra\u00eener le r\u00e9seau avec CIFAR10. \n\nIl est possible de seulement entra\u00eener la t\u00eate du r\u00e9seau ou bien d'entra\u00eener seulement quelques couches. Le choix du nombres de couche \u00e0 entra\u00eener va influer sur la performance du mod\u00e8le ainsi que le temps d'entra\u00eenement. Entra\u00eener la t\u00eate peut faire en sorte qu'il est possible d'extraire les repr\u00e9sentations des exemples et donc permettre un entra\u00eenement tr\u00e8s rapides. Au contraire, entra\u00eener toutes les couches du r\u00e9seau peut donner un mod\u00e8le tr\u00e8s performant."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "import math\nimport torch\nimport numpy as np\nfrom torch import optim, nn\nfrom torchvision import transforms\nimport torchvision.models as models\nfrom torchvision.datasets.cifar import CIFAR10\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.nn.init import kaiming_normal_, constant_\n\n# New imports!\nfrom poutyne.framework import Model, ModelCheckpoint, Callback, CSVLogger, EarlyStopping, ReduceLROnPlateau\nfrom poutyne import torch_to_numpy\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.utils import make_grid\n\ntorch.manual_seed(42)\nnp.random.seed(42)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "# Training hyperparameters\ncuda_device = 0\ndevice = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\nbatch_size = 32\nlearning_rate = 0.01\nn_epoch = 5\nnum_classes = 10"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "def load_cifar10(download=False, path='./', transform=None):\n    \"\"\"Loads the cifar10 dataset.\n\n    :param download: Download the dataset\n    :param path: Folder to put the dataset\n    :return: The train and test dataset\n    \"\"\"\n    train_dataset = CIFAR10(path, train=True, download=download, transform=transform)\n    test_dataset = CIFAR10(path, train=False, download=download, transform=transform)\n    return train_dataset, test_dataset\n\n\ndef load_cifar10_with_validation_set(download=False, path='./', train_split=0.8):\n    \"\"\"Loads the CIFAR10 dataset.\n\n    :param download: Download the dataset\n    :param path: Folder to put the dataset\n    :return: The train, valid and test dataset ready to be ingest in a neural network\n    \"\"\"\n    train, test = load_cifar10(download, path)\n    lengths = [round(train_split*len(train)), round((1.0-train_split)*len(train))]\n    train, valid = random_split(train, lengths)\n    return train, valid, test"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "norm_coefs = {}\nnorm_coefs['imagenet'] = [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(*norm_coefs['imagenet'])\n])\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ColorJitter(hue=.05, saturation=.05),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ToTensor(),\n    transforms.Normalize(*norm_coefs['imagenet'])\n])\n\ntrain, valid, test = load_cifar10_with_validation_set(download=True)\n\ntrain.dataset.transform = train_transforms\nvalid.dataset.transform = test_transforms\ntest.transform = test_transforms"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "len(train), len(valid), len(test)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid, batch_size=batch_size)\ntest_loader = DataLoader(test, batch_size=batch_size)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "def train(name, network, params=None):\n    if not params:\n        params = network.parameters()\n    \n    optimizer = optim.SGD(params, lr=learning_rate, momentum=0.9)\n    loss_function = nn.CrossEntropyLoss()\n    \n    early_stopping = EarlyStopping(patience=10, verbose=True)\n    lr_scheduler = ReduceLROnPlateau(patience=5, verbose=True)\n    callbacks = [early_stopping, lr_scheduler]\n\n    # Poutyne Model\n    model = Model(network, optimizer, loss_function, batch_metrics=['accuracy'])\n\n    # Send model on GPU\n    model.to(device)\n\n    # Train\n    model.fit_generator(train_loader, valid_loader, epochs=n_epoch, callbacks=callbacks)\n    return model"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## Nous allons entra\u00eener seulement la derni\u00e8re couche pour classifier 10 classes.\n\nNous devons donc red\u00e9finir la derni\u00e8re couche du r\u00e9seau ResNet-34 pour le bon nombre de classes dans notre jeu de donn\u00e9es (10 au lieu de 1000)."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "scrolled": true, "trusted": true}, "outputs": [], "source": "# Chargement du ResNet de 34 couches avec ses poids pr\u00e9-entra\u00een\u00e9s sur ImageNet\nnet = models.resnet34(pretrained=True)\n\n# Remplacement de la couche de classification\nnet.fc = nn.Linear(net.fc.in_features, num_classes)\n\nnet"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "list(net.named_parameters())"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Pour entra\u00eener seulement la derni\u00e8re couche, en PyTorch, nous pouvons seulement envoyer les param\u00e8tres de cette couche \u00e0 l'optimiseur.\n\nLes autres param\u00e8tres resteront inchang\u00e9s.\n\nNous en profitons pour bien initialiser ces nouveaux param\u00e8tres."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "def get_lr_for_last_layer_only(net):\n    # Filter params\n    classification_layer_params = [(n, p) for n, p in net.named_parameters() if 'fc' in n]\n    \n    # Initialize those\n    for n, p in classification_layer_params:\n        if 'weight' in n:\n            kaiming_normal_(p)\n        if 'bias' in n:\n            constant_(p, 0)\n    \n    # Return the list of different params/learning rates\n    classification_layer_params = [p for _, p in classification_layer_params]\n    return [\n        {'params': classification_layer_params, 'lr': 1e-2, 'momentum':0.9},\n    ]\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "params = get_lr_for_last_layer_only(net)"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "model = train('deep_net', net, params)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## Ici nous allons entra\u00eener la derni\u00e8re couche et peaufiner l'ensemble du r\u00e9seau.\n\nM\u00eame principe que l'\u00e9tape pr\u00e9c\u00e9dente, mais nous allons sp\u00e9cifier diff\u00e9rents taux d'apprentissage. Les couches pr\u00e9-entra\u00een\u00e9es auront un taux d'apprentissage plus petit alors que la nouvelle couche de classification aura un taux r\u00e9gulier."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "def get_lr_for_last_layer_and_fine_tune_conv(net):\n    # Filter params\n    classification_layer_params = [(n, p) for n, p in net.named_parameters() if 'fc' in n]\n    convolutional_layer_params = [p for n, p in net.named_parameters() if 'fc' not in n]\n    \n    # Initialize those\n    for n, p in classification_layer_params:\n        if 'weight' in n:\n            kaiming_normal_(p)\n        if 'bias' in n:\n            constant_(p, 0)\n    \n    # Return the list of different params/learning rates\n    classification_layer_params = [p for _, p in classification_layer_params]\n    return [\n        {'params': classification_layer_params, 'lr': 1e-2, 'momentum':0.9},\n        {'params': convolutional_layer_params, 'lr': 1e-4, 'momentum':0.9},\n    ]"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "net = models.resnet34(pretrained=True)\nnet.fc = nn.Linear(net.fc.in_features, num_classes)\nparams = get_lr_for_last_layer_and_fine_tune_conv(net)\nmodel = train('deep_net', net, params)"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "## Nous pouvons aussi effectuer un apprentissage complet du r\u00e9seau et voir si les r\u00e9sultats s'am\u00e9liorent."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "trusted": true}, "outputs": [], "source": "net = models.resnet34(pretrained=True)\nnet.fc = nn.Linear(net.fc.in_features, num_classes)\nmodel = train('deep_net', net)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "-4oZQu24WPIv", "trusted": true}, "outputs": [], "source": ""}], "metadata": {"PAX": {"revision": 938, "userLang": "fr"}, "accelerator": "GPU", "celltoolbar": "", "colab": {"collapsed_sections": [], "name": "Tutoriel 1.ipynb", "provenance": [], "version": "0.3.2"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.1"}, "revision": 938}, "nbformat": 4, "nbformat_minor": 1}