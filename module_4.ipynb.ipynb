{"cells": [{"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "CDEWTP-vNB_R"}, "source": "---\n# Pr\u00e9paration des Donn\u00e9es\n\n---\n\nPour pr\u00e9parer ad\u00e9quatement les donn\u00e9es avant de les fournir \u00e0 un ou plusieurs algorithmes d'apprentissage automatique, il faut \u00e9galement s'assurer de la bonne repr\u00e9sentation de ces donn\u00e9es pour ne pas leur fournir de valeurs trop diff\u00e9rentes qu'ils ne pourraient manipuler conjointement.\n\n<center><img src=\"https://python.gel.ulaval.ca/media/sio-u009/mlprocess_2.png\" alt=\"Processus d'apprentissage automatique\" width=\"50%\"/></center>\n\n1. Le nettoyage et les aberrations statistiques\n2. L'imputation de donn\u00e9es manquantes\n3. \u00c9quilibrage de donn\u00e9es d\u00e9s\u00e9quilibr\u00e9es\n4. **Transformation des caract\u00e9ristiques**\n    1. *rescaling* et *normalizing* (\\[0, 1\\] ou \\[-1, 1\\]), *standardizing* (loi normale)\n    2. Repr\u00e9sentation matricielle de donn\u00e9es cat\u00e9goris\u00e9es\n    3. R\u00e9duction de la dimensionnalit\u00e9 ou cr\u00e9ation de caract\u00e9ristiques\n\n\n\n"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "Ht4Pw2OeJYGi"}, "source": "## 4. Transformation des donn\u00e9es\n\n### R\u00e9\u00e9chelonnage ou normalisation\n\nS\u00e9quence inspir\u00e9e des exemples de code de la librairie `scikit-learn` de [Raghav RV](mailto:rvraghav93@gmail.com), [Guillaume Lemaitre](mailto:g.lemaitre58@gmail.com) et Thomas Unterthiner (License: BSD 3 clause)\n\nLe code ci-dessous est pour afficher les graphiques joliment"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "NTVBD3RCJYGi", "trusted": true}, "outputs": [], "source": "import numpy as np\nimport pandas as pd\n\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm\n\nprint(__doc__)\n\ncmap = getattr(cm, 'jet_r', cm.jet_r)\n\ndef create_axes(title, figsize=(16, 6)):\n    fig = plt.figure(figsize=figsize)\n    fig.suptitle(title)\n\n    # define the axis for the first plot\n    left, width = 0.1, 0.22\n    bottom, height = 0.1, 0.7\n    bottom_h = height + 0.15\n    left_h = left + width + 0.02\n\n    rect_scatter = [left, bottom, width, height]\n    rect_histx = [left, bottom_h, width, 0.1]\n    rect_histy = [left_h, bottom, 0.05, height]\n\n    ax_scatter = plt.axes(rect_scatter)\n    ax_histx = plt.axes(rect_histx)\n    ax_histy = plt.axes(rect_histy)\n\n    # define the axis for the zoomed-in plot\n    left = width + left + 0.2\n    left_h = left + width + 0.02\n\n    rect_scatter = [left, bottom, width, height]\n    rect_histx = [left, bottom_h, width, 0.1]\n    rect_histy = [left_h, bottom, 0.05, height]\n\n    ax_scatter_zoom = plt.axes(rect_scatter)\n    ax_histx_zoom = plt.axes(rect_histx)\n    ax_histy_zoom = plt.axes(rect_histy)\n\n    # define the axis for the colorbar\n    left, width = width + left + 0.13, 0.01\n\n    rect_colorbar = [left, bottom, width, height]\n    ax_colorbar = plt.axes(rect_colorbar)\n\n    return ((ax_scatter, ax_histy, ax_histx),\n            (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom),\n            ax_colorbar)\n\n\ndef plot_distribution(axes, X, y, hist_nbins=50, title=\"\", x0_label=\"\", x1_label=\"\"):\n    ax, hist_X1, hist_X0 = axes\n\n    ax.set_title(title)\n    ax.set_xlabel(x0_label)\n    ax.set_ylabel(x1_label)\n\n    # The scatter plot\n    colors = cmap(y)\n    ax.scatter(X[:, 0], X[:, 1], alpha=0.5, marker='o', s=5, lw=0, c=colors)\n\n    # Histogramme pour axe X1 (feature 5)\n    hist_X1.set_ylim(ax.get_ylim())\n    hist_X1.hist(X[:, 1], bins=hist_nbins, orientation='horizontal', color='grey', ec='grey')\n    hist_X1.axis('off')\n\n    # Histogramme pour axe X0 (feature 0)\n    hist_X0.set_xlim(ax.get_xlim())\n    hist_X0.hist(X[:, 0], bins=hist_nbins, orientation='vertical', color='grey', ec='grey')\n    hist_X0.axis('off')\n\ndef make_plot(title, X):\n    ax_zoom_out, ax_zoom_in, ax_colorbar = create_axes(title)\n    axarr = (ax_zoom_out, ax_zoom_in)\n    plot_distribution(axarr[0], X, y, hist_nbins=200,\n                      x0_label=\"Revenu m\u00e9dian\",\n                      x1_label=\"Nombre d'occupants\",\n                      title=\"Donn\u00e9es compl\u00e8tes\")\n\n    # On enleve les outliers \n    zoom_in_percentile_range = (0, 99)\n    cutoffs_X0 = np.percentile(X[:, 0], zoom_in_percentile_range)\n    cutoffs_X1 = np.percentile(X[:, 1], zoom_in_percentile_range)\n\n    non_outliers_mask = (\n        np.all(X > [cutoffs_X0[0], cutoffs_X1[0]], axis=1) &\n        np.all(X < [cutoffs_X0[1], cutoffs_X1[1]], axis=1))\n    plot_distribution(axarr[1], X[non_outliers_mask], y[non_outliers_mask],\n                      hist_nbins=200,\n                      x0_label=\"Revenu m\u00e9dian\",\n                      x1_label=\"Nombre d'occupants\",\n                      title=\"Sans Outliers\")\n\n    norm = mpl.colors.Normalize(y_full.min(), y_full.max())\n    mpl.colorbar.ColorbarBase(ax_colorbar, cmap=cmap,\n                              norm=norm, orientation='vertical',\n                              label='Valeur de la maison *100 K$')\n\n"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "za0yMaskJYGi"}, "source": "#### Donn\u00e9es brutes\n\nSi on ne touche \u00e0 rien cela donne ceci:"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "Om3vuso2JYGi", "trusted": true}, "outputs": [], "source": "from sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import minmax_scale\n# Donn\u00e9es des maisons californiennes : La valeur \u00e0 pr\u00e9dire est la valeur de la maison en centaine de milliers de dollars US\ndataset = fetch_california_housing()\nX_full, y_full = dataset.data, dataset.target\n#print(dataset.DESCR)\n# On ne prends que 2 caract\u00e9ristiques pour faciliter la visualisation : la 0 est distribution 'heavy-tailed', et la 5 a quelques donn\u00e9es aberrantes tr\u00e8s prononc\u00e9es\nX = X_full[:, [0, 5]]\n\n# # On re\u00e9chelonne la donn\u00e9e de sortie entre 0 et 1 pour mieux visualiser la l\u00e9gende (\u00e9taler la couleur)\ny = minmax_scale(y_full)\n\ndata = ('Unscaled data', X)\nmake_plot(*data)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "7AXxYT4EJYGk"}, "source": "#### Standardisation \n\nPour rappel : $x_i = \\frac{x_i - \\bar{x}}{\\sigma(x)}$"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "xViPlXRGJYGk", "trusted": true}, "outputs": [], "source": "from sklearn.preprocessing import StandardScaler\ndata = ('Data after standard scaling', StandardScaler().fit_transform(X))\nmake_plot(*data)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "GWlKqRwkJYGl"}, "source": "#### Min-Max\n\nPour rappel : $x_i = \\frac{x_i - \\min_i{x_i}}{\\max_i{x_i}-\\min_i{x_i}}$"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "YIrDQxvLJYGl", "trusted": true}, "outputs": [], "source": "from sklearn.preprocessing import MinMaxScaler\ndata = ('Data after min-max scaling', MinMaxScaler().fit_transform(X))\nmake_plot(*data)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "m5b1kbeaJYGn"}, "source": "#### Max-Abs\n\nPour rappel : $x_i = \\frac{x_i}{\\max_i{|x_i|}}$"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "gB75A7Z9JYGn", "trusted": true}, "outputs": [], "source": "from sklearn.preprocessing import MaxAbsScaler\ndata = ('Data after max-abs scaling', MaxAbsScaler().fit_transform(X))\nmake_plot(*data)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "i0JmNjJxJYGo"}, "source": "#### Robuste (bas\u00e9 sur les quantiles 1 et 3)\n\nPour rappel : $x_i = \\frac{x_i - Q_1(x)}{Q_3(x) - Q_1(x)}$\n\nLe centrage et la mise \u00e0 l'\u00e9chelle s'effectuent ind\u00e9pendamment sur chaque caract\u00e9ristique en calculant les statistiques pertinentes sur les \u00e9chantillons de l'ensemble d'apprentissage. La m\u00e9diane et l'intervalle interquartile sont ensuite stock\u00e9es pour \u00eatre utilis\u00e9es sur des donn\u00e9es ult\u00e9rieures \u00e0 l'aide de la m\u00e9thode de transformation.\n\nLa standardisation est une pratique courante pour de nombreux estimateurs d'apprentissage automatique. Cela se fait g\u00e9n\u00e9ralement en supprimant la moyenne et en divisant par la variance. Cependant, les valeurs aberrantes peuvent souvent influencer n\u00e9gativement la moyenne/variance de l'\u00e9chantillon. Dans ce cas, la m\u00e9diane et l\u2019intervalle interquartile donnent souvent de meilleurs r\u00e9sultats."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "NqR8AwNAJYGo", "trusted": true}, "outputs": [], "source": "from sklearn.preprocessing import RobustScaler\ndata = ('Data after robust scaling', RobustScaler(quantile_range=(25, 75)).fit_transform(X))\nmake_plot(*data)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "atimI0rKJYGp"}, "source": "#### Transformation par puissance\n\nFamille de transformations param\u00e9triques et monotones appliqu\u00e9es pour rendre les donn\u00e9es plus semblables \u00e0 celles de Gauss. Ceci est utile pour mod\u00e9liser les probl\u00e8mes li\u00e9s \u00e0 l'h\u00e9t\u00e9rosc\u00e9dasticit\u00e9 (variance non constante) ou \u00e0 d'autres situations dans lesquelles une normalit\u00e9 est souhait\u00e9e."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "z0Ka14HIJYGp", "trusted": true}, "outputs": [], "source": "from sklearn.preprocessing import PowerTransformer\ndata = ('Donn\u00e9es apr\u00e8s transformation par puissance (Yeo-Johnson)', PowerTransformer(method='yeo-johnson').fit_transform(X))\nmake_plot(*data)\ndata = ('Donn\u00e9es apr\u00e8s transformation par puissance (Box-Cox)', PowerTransformer(method='box-cox').fit_transform(X))\nmake_plot(*data)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "X3wRLD37JYGq"}, "source": "#### Transformation par Quantile\n\nLa transformation est appliqu\u00e9e \u00e0 chaque entit\u00e9 ind\u00e9pendamment. La fonction de densit\u00e9 cumul\u00e9e d'une entit\u00e9 est utilis\u00e9e pour projeter les valeurs d'origine. Les valeurs de caract\u00e9ristiques des donn\u00e9es nouvelles / invisibles qui sont inf\u00e9rieures ou sup\u00e9rieures \u00e0 la plage ajust\u00e9e seront mapp\u00e9es aux limites de la distribution en sortie. Notez que cette transformation est non lin\u00e9aire. Cela peut fausser les corr\u00e9lations lin\u00e9aires entre les variables mesur\u00e9es \u00e0 la m\u00eame \u00e9chelle, mais rend les variables mesur\u00e9es \u00e0 diff\u00e9rentes \u00e9chelles plus directement comparables."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "HHLnrZ6VJYGq", "trusted": true}, "outputs": [], "source": "from sklearn.preprocessing import QuantileTransformer\n\ndata = ('Donn\u00e9es apr\u00e8s transformation par quantile (gaussian)', QuantileTransformer(output_distribution='normal').fit_transform(X))\nmake_plot(*data)\ndata = ('Donn\u00e9es apr\u00e8s transformation par quantile (uniforme)', QuantileTransformer(output_distribution='uniform').fit_transform(X))\nmake_plot(*data)\n"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "5itXQozkJYGr"}, "source": "#### Normalisation \n \nPour rappel : $x_i = \\frac{x_i}{ ||x_i|| }$\n\nLa mise \u00e0 l'\u00e9chelle des entr\u00e9es en fonction des normes de l'unit\u00e9 est une op\u00e9ration courante pour la classification de texte ou la mise en cluster, par exemple. Par exemple, le produit scalaire de deux vecteurs TF-IDF normalis\u00e9s en $L_2$ est la similarit\u00e9 cosinus des vecteurs et constitue la m\u00e9trique de similarit\u00e9 de base pour le mod\u00e8le d'espace vectoriel couramment utilis\u00e9 par la communaut\u00e9 de r\u00e9cup\u00e9ration d'informations."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "gHG7tmsoJYGr", "trusted": true}, "outputs": [], "source": "from sklearn.preprocessing import Normalizer\ndata = ('Donn\u00e9es apr\u00e8s transformation par normalisation L2', Normalizer().fit_transform(X))\nmake_plot(*data)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "q0Z9iEOOJYGt"}, "source": "#### Une autre vision de ces r\u00e9\u00e9chantillonneurs\n\nPour mieux comprendre la diff\u00e9rence entre eux voici une visualisation de la caract\u00e9ristique `Revenu m\u00e9dian` avant et apr\u00e8s transformation."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "JDkYxRVFJYGu", "trusted": true}, "outputs": [], "source": "dfX = pd.DataFrame(dataset.data, columns=dataset.feature_names)\ndfX = dfX.drop(dataset.feature_names[1:],axis=1)\n\ncol = dfX.MedInc.values.reshape(-1, 1)\n\nscalers = [\n    ('Standard', StandardScaler()),\n    ('Min-Max', MinMaxScaler()),\n    ('Max-Abs', MaxAbsScaler()),\n    ('Robuste', RobustScaler(quantile_range=(25, 75))),\n    ('Quantile (gaussian)', QuantileTransformer(output_distribution='normal')),\n    ('Quantile (uniforme)', QuantileTransformer(output_distribution='uniform'))\n]\n\nfor scaler in scalers:\n    dfX[scaler[0]] = scaler[1].fit_transform(col)\n    \n\ndfX.describe()"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "GgA3VpU9JYGu", "trusted": true}, "outputs": [], "source": "import seaborn as sns\nsns.set(style=\"white\", color_codes=True)\n\nplt.figure(figsize=(15,5))\n\ncpt = 1\nf = plt.figure(figsize=(30,12))\nfor scaler in scalers:\n    name = scaler[0]\n    ax = f.add_subplot(2,len(scalers),cpt)\n    \n    sns.distplot(dfX.MedInc,ax=ax)\n    ax.axvline(dfX.MedInc.mean(), color='k', linestyle='dashed', linewidth=1)\n    sns.distplot(dfX[name],ax=ax)\n    ax.axvline(dfX[name].mean(), color='k', linestyle='dashed', linewidth=1)\n    ax.set_title(name)\n\n    ax = f.add_subplot(2,len(scalers),cpt+len(scalers))\n    g = sns.regplot(x=\"MedInc\", y=name, data=dfX,ax=ax)\n    cpt+=1"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "r45ck05CJYGw"}, "source": "### R\u00e9duction de la dimensionnalit\u00e9\n\nS\u00e9quence inspir\u00e9e des exemples de code de la librairie `scikit-learn` de [Fabian Pedregosa](mailto:fabian.pedregosa@inria.fr), [Olivier Grisel](mailto:olivier.grisel@ensta.org), [Mathieu Blondel](mailto:mathieu@mblondel.org), et Gael Varoquaux (License: BSD 3 clause)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "8XIDUI1lJYGw", "trusted": true}, "outputs": [], "source": "print(__doc__)\nfrom time import time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import offsetbox\nfrom sklearn import (manifold, datasets, decomposition, ensemble,\n                     discriminant_analysis, random_projection)\n\ndigits = datasets.load_digits(n_class=6)\nX = digits.data\ny = digits.target\nn_samples, n_features = X.shape\nn_neighbors = 30\nn_components = 2\n\n# On d\u00e9finit une fonction pour adfficher la projection\ndef plot_embedding(X, title=None):\n    x_min, x_max = np.min(X, 0), np.max(X, 0)\n    X = (X - x_min) / (x_max - x_min)\n\n    plt.figure()\n    ax = plt.subplot(111)\n    for i in range(X.shape[0]):\n        plt.text(X[i, 0], X[i, 1], str(y[i]),\n                 color=plt.cm.Set1((y[i]+1) / 10.),\n                 fontdict={'weight': 'bold', 'size': 9})\n    plt.xticks([]), plt.yticks([])\n    if title is not None:\n        plt.title(title)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "YXGCipfPJYGw"}, "source": "#### Exemples de donn\u00e9es"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "_gAYdoLcJYGw", "trusted": true}, "outputs": [], "source": "# Afficher quelques images de chiffres pour l'exemple\nn_img_per_row = 20\nimg = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))\nfor i in range(n_img_per_row):\n    ix = 10 * i + 1\n    for j in range(n_img_per_row):\n        iy = 10 * j + 1\n        img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))\n\nplt.imshow(img, cmap=plt.cm.binary)\nplt.xticks([])\nplt.yticks([])\nplt.title('Une s\u00e9lection des 5 premiers chiffres de MNIST')"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "nfyjOVCdJYGx"}, "source": "#### Projection al\u00e9atoire"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "2VENt9jfJYGy", "trusted": true}, "outputs": [], "source": "%%time\n# Projection al\u00e9atoire 2D (basiquement m\u00e9langer) \u00e0 partir d'une matrice al\u00e9toire\nrp = random_projection.SparseRandomProjection(n_components=n_components, random_state=42)\nX_projected = rp.fit_transform(X)\nplot_embedding(X_projected, \"Projection al\u00e9atoire des chiffres\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "0yxuSBgIJYGz"}, "source": "#### Analyse en composante principales (PCA)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "esYaykJAJYGz", "trusted": true}, "outputs": [], "source": "%%time\n# Projection des chiffres par les 2 principaux composants sur la matrice de covariance\nX_pca = decomposition.PCA(n_components=n_components).fit_transform(X)\nplot_embedding(X_pca, \"Projection des chiffres par Composante Principale\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "NZykhCl2JYG0"}, "source": "#### PCA pour donn\u00e9es sparses (Latent Sementic)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "Zt9QtuKOJYG0", "trusted": true}, "outputs": [], "source": "%%time\n# Projection des chiffres par les 2 principaux composants LSA (latent semantic analysis) = PCA pour donn\u00e9es sparse.\nX_pca = decomposition.TruncatedSVD(n_components=n_components).fit_transform(X)\nplot_embedding(X_pca, \"Projection des chiffres par Composante Principale\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "F1qaYHAPJYG1"}, "source": "#### Analyse par discriminants lin\u00e9aires (LDA)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "jH64uvBUJYG1", "trusted": true}, "outputs": [], "source": "%%time\n# Projection des chiffres par les deux principaux discriminants lin\u00e9aires\nX2 = X.copy()\nX2.flat[::X.shape[1] + 1] += 0.01  # X doit \u00eatre inversable ... \nX_lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=n_components).fit_transform(X2, y)\nplot_embedding(X_lda, \"Projection des chiffres par Discrimination Lin\u00e9aire\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "XkFvgK-QJYG2"}, "source": "#### Isomap"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "HE65NqZQJYG2", "trusted": true}, "outputs": [], "source": "%%time\n# Projection des chiffres par Isomap\nX_iso = manifold.Isomap(n_neighbors, n_components=n_components).fit_transform(X)\nplot_embedding(X_iso, \"Projection des chiffres par Isomap\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "KmpN_sHDJYG3"}, "source": "#### Encodage Locallement Lin\u00e9aire (LLE)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "4tOkHjwrJYG4", "trusted": true}, "outputs": [], "source": "%%time\n# Encodage localement lin\u00e9aire des chiffres\nclf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=n_components, method='standard')\nX_lle = clf.fit_transform(X)\nplot_embedding(X_lle, \"Encodage localement lon\u00e9aire des chiffres\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "2dthP03yJYG4"}, "source": "#### Alignement des tangentes locales (LSTA)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "jHvZPrGAJYG4", "trusted": true}, "outputs": [], "source": "%%time\n# LTSA : Alignement des tangentes locales des chiffres\nclf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=n_components, method='ltsa')\nX_ltsa = clf.fit_transform(X)\nplot_embedding(X_ltsa,\n               \"LTSA : Alignement des tangentes locales des chiffres\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "1t0Y6DtaJYG6"}, "source": "#### Multidimensional Scaling (MDS)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "nwzus0a0JYG6", "trusted": true}, "outputs": [], "source": "%%time\n# Encodage MDS des chiffres\nclf = manifold.MDS(n_components=n_components, n_init=1, max_iter=100)\nX_mds = clf.fit_transform(X)\nplot_embedding(X_mds, \"Encodage MDS des chiffres\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "FYYtdIVZJYG7"}, "source": "#### For\u00eat d'Isolation"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "Av_07JZOJYG7", "trusted": true}, "outputs": [], "source": "%%time\n# Encodage par arbres al\u00e9atoires des chiffres\nhasher = ensemble.RandomTreesEmbedding(n_estimators=200, random_state=0, max_depth=5)\nX_transformed = hasher.fit_transform(X)\npca = decomposition.TruncatedSVD(n_components=n_components)\nX_reduced = pca.fit_transform(X_transformed)\n\nplot_embedding(X_reduced, \"Encodage par arbres al\u00e9atoires des chiffres\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "860d-R_tJYG8"}, "source": "#### Encodage Spectral"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "x16dC-t4JYG8", "trusted": true}, "outputs": [], "source": "%%time\n# Encodage Spectral des chiffres\nembedder = manifold.SpectralEmbedding(n_components=n_components, random_state=0, eigen_solver=\"arpack\")\nX_se = embedder.fit_transform(X)\n\nplot_embedding(X_se,\"Encodage Spectral des chiffres\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "hp2iggjyJYG9"}, "source": "#### t-SNE"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "rJDQH1i0JYG9", "trusted": true}, "outputs": [], "source": "%%time\n# Encodage t-SNE des chiffres\n# Visualizing : https://github.com/oreillymedia/t-SNE-tutorial\ntsne = manifold.TSNE(n_components=n_components, init='pca', random_state=0)\nX_tsne = tsne.fit_transform(X)\n\nplot_embedding(X_tsne,\"Encodage t-SNE des chiffres\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "1Y8wPwYgJYHJ"}, "source": "### Variables cat\u00e9goriques\n\n**Regroupement de cat\u00e9gories en une seule valeur:**\n\nExemple: Vous avez une variable cat\u00e9gorielle `pays` qui a 180 valeurs uniques. Vous voulez voir les 3 pays les plus importants seulement et placer tous les autres pays dans une valeur \"autre\".\n\n```\n# Afficher le nombre de valeurs uniques de chaque variable cat\u00e9gorique dans les donn\u00e9es\nfor i in df.columns:\n    if df[i].dtypes=='object': \n        unique_cat=len(df[i].unique())\n        print(\"La carct\u00e9ristique '{i}' a {unique_cat} cat\u00e9gories uniques\".format(col_name=col_name, unique_cat=unique_cat))\n\n# Afficher le nombre de valeurs de chaque variable cat\u00e9gorique\nprint(df['pays'].value_counts())\n\n# Cat\u00e9goriser les cat\u00e9gories moins fr\u00e9quentes comme \"autres\"\ndef repl(x):\n    if x == 'US': return 'US'\n    elif x == 'BR': return 'BR'\n    elif x == 'ES': return 'ES'\n    else: return 'Other'\n    \ndf['pays'] = df['COUNTRY'].apply(repl)\nprint(df['pays'].value_counts().sort_values(ascending=False))\n```\n\n**Regroupement de variables num\u00e9riques en cat\u00e9gories**\n\n**Cat\u00e9gorie de taille \u00e9gales :**\n\n```\ndf['var'] = 0\nvar = pd.qcut(x=COL1, q=3, labels=[\"good\", \"medium\", \"bad\"])\nprint(df['var'].value_counts().sort_values(ascending=False))\n```\nAjustez le nombre de cat\u00e9gories (q) et les \u00e9tiquettes.\n\n**Cat\u00e9gories par intervalles identiques :**\n\n```\nbins = [0, 20, 40, 60, 80, 100] \ndf.var = pd.cut(x=COL1, bins, labels=['Very low', 'Low', 'Medium', 'High', 'Very high']) \nprint(data['var'].value_counts().sort_values(ascending = False))\n```\nAjustez les cat\u00e9gories en sp\u00e9cifiant les seuils (doit avoir un de plus que le nombre de cat\u00e9gories/\u00e9tiquettes). Par d\u00e9faut, les emplacements incluent le bord le plus \u00e0 droite, d\u00e9finissez l'argument `right = False` si les limites de droite ne doivent pas \u00eatre inclues.\n\n### Encoder des variables categoriques\n\n**Encoder une variable bool\u00e9enne en la transformant en entier :**\n\n```\ndf['BOOL'] = (df.COL1==\"ABC\").astype(int)\ndta.head()\n```\nDans cet exemple, COL1 contient des valeurs de cha\u00eene. BOOL sera \u00e9gal \u00e0 1 si COL1 contient \"ABC\".\n\n**Encoder manuellement en mappant un dictionnaire :**\n\n```\ndic = {'Yes': 1, 'No': 2}\ndf['VAR'] = df['VAR'].map(dic)\n```\n\n**Encoder automatiquement (OneHot) :**\n\n```\nfrom sklearn.preprocessing import OneHotEncoder\n\nonehotencoder = OneHotEncoder(categorical_features = [0])\nx = onehotencoder.fit_transform(x).toarray()\n\n```\nLa colonne devant \u00eatre encod\u00e9e est spc\u00e9cifi\u00e9es dans le constructeur, [0] dans l'exemple. Les donn\u00e9es `x` sont ensuite transform\u00e9es avec l\u2019objet onehotencoder. Apr\u00e8s ces \u00e9tapes il y a maintenant autant de nouvelles colonnes dans le jeu de donn\u00e9es que de cat\u00e9gories dans la colonne [0].\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "6NHwuOSQJYHJ", "trusted": true}, "outputs": [], "source": ""}], "metadata": {"PAX": {"revision": 820, "userLang": "fr"}, "celltoolbar": "", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.1"}, "revision": 820, "varInspector": {"cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"delete_cmd_postfix": "", "delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())"}, "r": {"delete_cmd_postfix": ") ", "delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": false}}, "nbformat": 4, "nbformat_minor": 1}