{"cells": [{"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "5Blfm7PRJYA1"}, "source": "---\n# Introduction \u00e0 l'apprentissage automatique\n---\n\n<center><img src=\"https://python.gel.ulaval.ca/media/sio-u009/mlprocess_3.png\" alt=\"Processus d'apprentissage automatique\" width=\"50%\"/></center>\n\n# Exemple 1 : Composantes d'un algorithme\n\nDans cet exercice, l'objectif est de d\u00e9finir et comprendre le vocabulaire utilis\u00e9 en apprentissage automatique et ce qu'il repr\u00e9sente. Nous allons voir l'ensemble des donn\u00e9es (_data set_), la fonction de perte (_loss function_), la diff\u00e9rence entre le vrai rique et le risque empirique, la r\u00e9gularisation, la fonction objectif (_objective function_).\n\n\nImport des librairies n\u00e9cessaires"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "1LEstp5hJYA3", "trusted": true}, "outputs": [], "source": "%matplotlib inline\n# %matplotlib notebook # Pour la maniupulation des images 3D\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "EoZYXqvfJYA-"}, "source": "Choix de la fonction \u00e0 apprendre pour l'exemple : ici un polynome de degr\u00e9 5."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "_ZKNHsgvJYBB", "trusted": true}, "outputs": [], "source": "listeParamPoly = [0.03, 0.2, -1, -10, 100]"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "EuHPZ_YgJYBH", "trusted": true}, "outputs": [], "source": "def generate_data(N):\n    x = np.random.uniform(-10,10,N)\n    y = np.polyval(listeParamPoly,x) + np.random.normal(0.0, 15.0, N)\n    return x.reshape(-1, 1), y"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "icKaGzgQJYBL"}, "source": "G\u00e9n\u00e9ration des donn\u00e9es : Choisir le nombre de points \u00e0 utiliser :"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "eSaJoHVHJYBM", "trusted": true}, "outputs": [], "source": "dataPoints = 20"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "kw6ONosKJYBR", "trusted": true}, "outputs": [], "source": "X,y = generate_data(dataPoints)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "wPtnQ6AiJYBV"}, "source": "## Visualisation des donn\u00e9es "}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "nUrlgn3UJYBW", "trusted": true}, "outputs": [], "source": "fig, ax = plt.subplots()\n# Affichage des points\nax.plot(X, y, 'o')\n# Affichage de la fonction \u00e9chantillon\u00e9e\nax.plot(np.linspace(-10,10,100), np.polyval(listeParamPoly,np.linspace(-10,10,100)), color='black', linewidth=3)\n# Axes & titre\nax.set_title('Data set')\nax.set_ylabel('y')\nax.set_xlabel('x')\nplt.show()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "OCn0Hw9jJYBa"}, "source": "## D\u00e9finition de la fonction de perte\n\nChoisir une fonction de perte parmi les trois donn\u00e9es dans la pr\u00e9sentation ... ou une autre au choix.\n\n$$\\mathcal{L}(\\hat{y},y) = |\\hat{y}-y|^2$$"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "oFLKSt9LJYBb", "trusted": true}, "outputs": [], "source": "L = lambda y1, y2: np.abs(y1-y2)**2"}, {"cell_type": "markdown", "metadata": {"editable": false}, "source": "Vous remarquerez que vous pouvez simplement passer de la perte quadratique \u00e0 la perte absolue en enlevant le `**2` \u00e0 la fin de la fonction et en r\u00e9\u00e9x\u00e9cutant la cellule."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "5THZJHRzJYBf"}, "source": "## D\u00e9finition du mod\u00e8le pr\u00e9dictif \n\nOn prend une r\u00e9gression lin\u00e9aire pour l'exemple, si vous souhaitez changer ca pour un autre mod\u00e8le c'est ici : \n$$\\hat{y} = \\theta_0\\cdot x + \\theta_1$$\n\nSi on souhaitait prendre un mod\u00e8le quadratique, on utiliserait : \n$$\\hat{y} = \\theta_0\\cdot x^2 + \\theta_1\\cdot x + \\theta_2$$\n\nN'h\u00e9sitez pas \u00e0 changer le code ici un fois rendu \u00e0 la fin de cet exercice pour voir ce que vous pourriez apprendre de mieux ;-)\n\nSeul souci \u00e0 pr\u00e9voir : vous ne pourrez plus faire la visualisation 3D plus bas, car on aurais besoin de faire une visualisation en 4 dimensions."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "9EQ5nLwDJYBf", "trusted": true}, "outputs": [], "source": "def h(x, theta):\n    y = theta[0]*x + theta[1]\n    return y"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "xIi11pV6JYBj"}, "source": "## La fonction de risque empirique\n\nLe risque empirique est calcul\u00e9 \u00e0 partir de la fonction de perte et du mod\u00e8le choisis sur les donn\u00e9es r\u00e9elles.\n\nIci, nous avons choisi de prendre la moyenne des erreurs $\\mathcal{L}(\\hat{y},y)$ (MAE, MSE, ...) mais d'autre formes existente avec le $\\min$ ou le $\\max$."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "3497aridJYBk", "trusted": true}, "outputs": [], "source": "def risque_empirique(L,y,X,theta):\n    loss = 0\n    for i in range(0,np.max(y.shape)):\n        y_hat = h(X[i], theta)\n        loss = loss + L(y[i], y_hat)\n    return loss/float(np.max(y.shape))"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "DoTKp1ONJYBn"}, "source": "## D\u00e9finition d'une fonction de r\u00e9gularisation\n\nIci une r\u00e9gularisation $L_1$ est choisie. Vous pouvez en prendre une autre (la $L_2$ par exemple)."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "wKV5KoJCJYBo", "trusted": true}, "outputs": [], "source": "def regularisation(theta):\n    return np.sum(np.abs(theta)) #L1\n#    return ??? #L2"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "_W45crbAJYBs"}, "source": "## La fonction objective\n\nSe d\u00e9finit comme la somme du risque empirique et de la r\u00e9gularisation"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "ca8K1gZFJYBs", "trusted": true}, "outputs": [], "source": "def objective(L,y,X,r,theta):\n    return risque_empirique(L,y,X,theta) + r*regularisation(theta)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "NopZwyfPJYBx"}, "source": "## Calcul exhaustif de la fonction objective (g\u00e9n\u00e9ralement intractable)\n\nOn calcule la fonction objectif pour tous les param\u00e8tres possibles. Avec deux ou trois param\u00e8tres c'est encore possible ... mais ce n'est visualisable qu'\u00e0 deux."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "BAOhLIcsJYBy", "trusted": true}, "outputs": [], "source": "M = 25\nlinspace_A = np.linspace(-200, 200, M)\nlinspace_b = np.linspace(-200, 200, M)\nR = np.zeros((M, M))\nr = 0.1\nfor i in range(0,M):\n    for j in range(0,M):\n        theta = np.array([linspace_A[i], linspace_b[j]])\n        R[i,j] = np.log(objective(L,y,X,r,theta)) "}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "z3qWBgwKJYB1"}, "source": "## Visualisation de la fonction objective\n\nAvec deux param\u00e8tres, ca se visualise bien. Comme dit tantot, si vous en avez mis plus, ceci ne pourra plus marcher."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "Yj3lJ6BtJYB2", "trusted": true}, "outputs": [], "source": "fig = plt.figure()\nax = plt.axes(projection='3d')\nmesh_A, mesh_b = np.meshgrid(linspace_A, linspace_b)\nax.plot_surface(mesh_A, mesh_b, R)\nplt.show()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "kny-YamYJYB5"}, "source": "## Optimisation avec scipy\n\nOn ne rentrera pas dans la boite (de Pandore) de l'optimisation dans cette formation. Nous allons utiliser un des outils de `scipy`, la m\u00e9thode [`nelder-mead`](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method)."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "_CHbsNRfJYB5", "trusted": true}, "outputs": [], "source": "from scipy.optimize import minimize"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "_ctQIZXDJYB9", "trusted": true}, "outputs": [], "source": "r = 0.1\ntheta0 = np.array([0.0, 0.0])\nf = lambda theta: objective(L,y,X,r,theta)\nes = minimize(f, theta0, method='nelder-mead', options={'xtol': 1e-8, 'disp': True})\ntheta_opt = es.x"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "zr0847LmJYCA", "trusted": true}, "outputs": [], "source": "print('Les param\u00e8tres estim\u00e9s sont :')\nprint('A = %2.4f, b = %2.4f' % (theta_opt[0], theta_opt[1]))"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "m01A-N4YJYCC"}, "source": "## Visualisation du mod\u00e8le appris"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "LhxqfRBBJYCD", "trusted": true}, "outputs": [], "source": "fig, ax = plt.subplots()\nax.plot(X, y, 'o')\nax.set_title('Polyn\u00f4me')\nax.set_ylabel('y')\nax.set_xlabel('x')\n\nlinspace_x = np.linspace(-10, 10, num=100)\nA = theta_opt[0]\nb = theta_opt[1]\nh_x = h(linspace_x,theta_opt)\nax.plot(linspace_x, h_x, color='red', linewidth=3)\n\nplt.show()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "ElfBb5nMJYCG"}, "source": "## Calcul du \"vrai\" risque\n\nEncore une fois, c'est rarement possible de le faire. Il faut avoir acc\u00e8s \u00e0 la distribution r\u00e9elle des donn\u00e9es sur laquelle on int\u00e9rroge notre mod\u00e8le jusqu'\u00e0 avoir une bonne estimation du risque. "}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "-Rm7ImRbJYCG", "trusted": true}, "outputs": [], "source": "X_test, y_test = generate_data(1000000)\nvrai_risque = risque_empirique(L,y_test,X_test,theta_opt)\nprint('Le vrai risque du mod\u00e8le appris est : %5.2f' % vrai_risque[0])"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "w7y2Eq4QJYCJ"}, "source": "# Exemple 2 : Augmentation de la capacit\u00e9 d'un mod\u00e8le"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "8nGaTxvXJYCK"}, "source": "## D\u00e9finition des fonctions de caract\u00e9ristiques\n\nAjoutez dans le vecteur $\\varphi$ autant de fonctions caract\u00e9ristiques que vous le souhaitez. Id\u00e9alement non-lin\u00e9aires."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "JkcG5RwjJYCK", "trusted": true}, "outputs": [], "source": "phi_names = [\n       \"x: x**0\", \n       #\"x: x**1\",\n       #...d'autres ?\n       #\"x: np.abs(x)\",\n       #\"x: x > 0.0\",\n       #...au choix ?\n       #\"x: np.cos(x)\",\n       #\"x: np.sin(0.1*x)\"\n       #...ou encore ?\n      ]\n\nphi = [\n       lambda x: x**0, #...en haut c'\u00e9tait juste les noms pour l'affichage ... il faut les coder maintenant !\n      ]"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "Y5Nj4IqJJYCO"}, "source": "On va utiliser ce vecteur pour faire la projection. Voici la fonction de projection:"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "z-r_XER_JYCO", "trusted": true}, "outputs": [], "source": "def feature_space_projection(X, phi):\n    X_features = []\n    for i in range(0, len(phi)):\n        X_features.append(np.apply_along_axis(phi[i], 0, X))\n    X_augmented = np.concatenate(X_features, axis=1)\n    return X_augmented"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "izs4N-YwJYCR"}, "source": "## Calcul de la projection dans l'espace des caract\u00e9ristiques\n\nCalcul de la projection puis affichage de notre entr\u00e9e dans le nouvel espace."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "svF6TZVfJYCR", "trusted": true}, "outputs": [], "source": "X_augmented = feature_space_projection(X, phi)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "DJEzfVicJYCV", "trusted": true}, "outputs": [], "source": "X_augmented[0,:]"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "nV80KgVmJYCX"}, "source": "## Entra\u00eenement du mod\u00e8le\n\nMaintenant qu'on a ouvert la bo\u00eete \u00e0 l'exercice pr\u00e9c\u00e9dent, on va utiliser le mod\u00e8le de regression lin\u00e9aire [`linear_model.LinearRegression()`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) de `scikit-learn`. On pourrait utiliser d'autres mod\u00e8les [lin\u00e9aires](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "GSI2VpNAJYCY", "trusted": true}, "outputs": [], "source": "from sklearn import linear_model\n\nreg = linear_model.LinearRegression()\nreg.fit(X_augmented, y)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "5sUMC-sDJYCb"}, "source": "## Calcul de l'erreur sur l'ensemble d'entra\u00eenement vs le \"vrai\" risque"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "rC4DMqbhJYCb", "trusted": true}, "outputs": [], "source": "from sklearn.metrics import mean_squared_error"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "Mq3z_FA6JYCe"}, "source": "### Erreur d'entra\u00eenement\n\nCalcul de la fonction de perte [MSE](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) sur les donn\u00e9es d'entrainement:"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "4dU45OtJJYCg", "trusted": true}, "outputs": [], "source": "y_pred = reg.predict(X_augmented)\ntraining_error = mean_squared_error(y, y_pred)\nprint(\"L'erreur d'entra\u00eenement du mod\u00e8le appris est : %5.2f\" % training_error)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "z2RFk8qjJYCi"}, "source": "### Estimation du vrai risque\n\nEt si on valide (non r\u00e9alistiquement) sur la *vraie* distribution des donn\u00e9es:"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "zImPA5LAJYCj", "trusted": true}, "outputs": [], "source": "X_test,y_test = generate_data(10000000)\nX_test_augmented = feature_space_projection(X_test, phi)\ny_test_pred = reg.predict(X_test_augmented)\ntest_error = mean_squared_error(y_test, y_test_pred)\nprint(\"Vrai risque du mod\u00e8le appris est : %5.2f\" % test_error)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "ZkNLSTc6JYCl"}, "source": "## Attention au sur-apprentissage\n\nLorsque le nombre de caract\u00e9ristiques utilis\u00e9 est tr\u00e8s important, la capacit\u00e9 augmente \u00e9norm\u00e9ment jusqu'\u00e0 apprendre *par coeur* les donn\u00e9es."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "i7aLEeK3JYCm", "trusted": true}, "outputs": [], "source": "fig, ax = plt.subplots()\nax.plot(X, y, 'o')\nax.set_title('Polyn\u00f4me')\nax.set_ylabel('y')\nax.set_xlabel('x')\n\nlinspace_x = np.linspace(-10, 10, num=100000)\nlinspace_x = np.expand_dims(linspace_x, axis=1)\n\nlinspace_X_augmented = feature_space_projection(linspace_x, phi)\n\ny_pred = reg.predict(linspace_X_augmented)\nax.plot(linspace_x, y_pred, color='red', linewidth=3)\n\nplt.show()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "8zn6HJOvJYCp"}, "source": "## R\u00e9gularisation et hyperparam\u00e8tres\n\nIl nous faut alors soit r\u00e9duire le nombres de caract\u00e9ristiques (et donc la dimensionnalit\u00e9 de l'espace de projection), soit *r\u00e9gulariser* nos param\u00e8tres pour assurer une bonne *g\u00e9n\u00e9ralisation*.\n\nOn va utiliser ici [ElasticNet](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet) de `scikit-learn` qui permet de combiner les normes $L_1$ et $L_2$ pour la r\u00e9gularisation.\n\nVous pouvez chosir ici le taux de r\u00e9gularisation et la proportion de r\u00e9gularisation $L_1$ vs $L_2$."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "B87unsXNJYCp", "trusted": true}, "outputs": [], "source": "taux_de_regularisation = 1.0\nratio_normes = 0.5"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "1lVZqQSPJYCr"}, "source": "On d\u00e9finit ensuite le mod\u00e8le et on l'entraine."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "pCGo4x7aJYCr", "trusted": true}, "outputs": [], "source": "reg2 = linear_model.ElasticNet(alpha=taux_de_regularisation, copy_X=True, fit_intercept=True, l1_ratio=ratio_normes,\n      max_iter=10000, normalize=False, positive=False, precompute=False,\n      random_state=None, selection='random', tol=0.0001, warm_start=False)\nreg2.fit(X_augmented, y)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "sbd5CKKKJYCt"}, "source": "Calcul du risque empirique : "}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "osTBeIq0JYCu", "trusted": true}, "outputs": [], "source": "y_pred = reg2.predict(X_augmented)\ntraining_error = mean_squared_error(y, y_pred)\nprint(\"L'erreur d'entra\u00eenement du mod\u00e8le appris est : %5.2f\" % training_error)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "bg0Lei9PJYCx"}, "source": "Calcul du *vrai* risque:"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "_jpYwV49JYCy", "trusted": true}, "outputs": [], "source": "X_test,y_test = generate_data(10000)\nX_test_augmented = feature_space_projection(X_test, phi)\ny_test_pred = reg2.predict(X_test_augmented)\ntest_error = mean_squared_error(y_test, y_test_pred)\nprint(\"Vrai risque du mod\u00e8le appris est : %5.2f\" % test_error)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "dn95IH9MJYC0"}, "source": "Affichage du mod\u00e8le r\u00e9gularis\u00e9 :"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "Ug-knbyZJYC0", "trusted": true}, "outputs": [], "source": "fig, ax = plt.subplots()\nax.plot(X, y, 'o')\nax.set_title('Polyn\u00f4me')\nax.set_ylabel('y')\nax.set_xlabel('x')\n\nlinspace_x = np.linspace(-10, 10, num=10000)\nlinspace_x = np.expand_dims(linspace_x, axis=1)\n\nlinspace_X_augmented = feature_space_projection(linspace_x, phi)\n\ny_pred = reg2.predict(linspace_X_augmented)\nax.plot(linspace_x, y_pred, color='red', linewidth=3)\n\nplt.show()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "xQAHyayxJYC2"}, "source": "Choix fait des param\u00e8tres de r\u00e9gularisation par ElasticNet:\n\n**Note :** Si vous avez des erreurs ici, assurez vous avant toute chose que le nombre de noms d\u00e9finis dans le dictionnaire `phi_names` est le m\u00eame que le nombre de fonctions d\u00e9finies dans le dictionnaire `phi`."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "7dsp59AGJYC3", "trusted": true}, "outputs": [], "source": "import pandas as pd\n\nprint('\\''+pd.DataFrame({'names':phi_names,'coefs':reg2.coef_}).to_string(index=False)[1:])"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "h2aZVuH3JYC5"}, "source": "**Pouvez vous identifier quels sont les fonctions de caract\u00e9ristiques inutiles ?**\n\nSi vous ne comprenez pas comment ou pourquoi, n'h\u00e9sitez pas \u00e0 venir en discuter dans le forum !"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "Umht3EKKJYC6"}, "source": "# Exemple 3 : M\u00e9thodologie de validation et test"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "sxkBSA5QJYC6", "trusted": true}, "outputs": [], "source": "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "IO_3B2SXJYC7", "trusted": true}, "outputs": [], "source": "def confidence_interval(y, y_pred):\n    n = len(y)\n    s = np.sqrt(np.var(mean_squared_error(np.expand_dims(y,1).transpose(), np.expand_dims(y_pred,1).transpose(),\n                          multioutput='raw_values'), ddof=1))\n    return 1.96*s/np.sqrt(n)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "VFS21KSWJYC-"}, "source": "(re)G\u00e9n\u00e9ration des donn\u00e9es, on va en prendre plus pour avoir de bons intervalles de confiance. Choisir le nombre de points \u00e0 utiliser :"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "g0tC6BOXJYC-", "trusted": true}, "outputs": [], "source": "dataPoints = 200"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "5E4bkjSrJYDA", "trusted": true}, "outputs": [], "source": "X,y = generate_data(dataPoints)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "g7UIluMUJYDJ", "trusted": true}, "outputs": [], "source": "fig, ax = plt.subplots()\nax.plot(X, y, 'o')\nax.plot(np.linspace(-10,10,100), np.polyval(listeParamPoly,np.linspace(-10,10,100)), color='black', linewidth=3)\nax.set_title('Data set')\nax.set_ylabel('y')\nax.set_xlabel('x')\nplt.show()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "zr_hjpSdJYDL"}, "source": "## D\u00e9finition des fonctions de caract\u00e9ristiques\n\nReprenez ici vos fonctions de caract\u00e9ristiques de l'exercice pr\u00e9c\u00e9dent."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "8vNXvaONJYDM", "trusted": true}, "outputs": [], "source": "# ??? Choisissez celles qui vous semble les plus pertinentes :-)\nphi = [\n    lambda x: x**0, #...Celle ci est pas tr\u00e8s utile il me semble ...\n    #...\n]"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "T2upVkX5JYDP"}, "source": "## Pr\u00e9paration des donn\u00e9es test\n\nApr\u00e8s projection dans l'espace des caract\u00e9ristiques, nous d\u00e9coupons notre dataset en trois parties."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "Ok2MjsvEJYDP", "trusted": true}, "outputs": [], "source": "X_phi = feature_space_projection(X, phi)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "Gv05TNbrJYDR"}, "source": "\n### splits : train(50%) - validation(25%) - test(25%)\n\nUn ensemble d'entrainement (_train_), un ensemble de _validation_ qui va nous permettre d'\u00e9valuer nos mod\u00e8les et de choisir notre pr\u00e9f\u00e9r\u00e9 sur un ensemble de donn\u00e9es jamais vu, et un ensemble de _test_ pour l'\u00e9valuation final en mode **r\u00e9el**.\n\nRemarquez que *th\u00e9oriquement*, l'ensemble de validation et l'ensemble de test sont identiques (sous l'hypoth\u00e8se [IID](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)) et peuvent \u00eatre permut\u00e9s sans probl\u00e8me pour la m\u00e9thodologie (mais au d\u00e9but seulement, apr\u00e8s il est interdit de toucher au _test_ avant la fin)."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "5SO-NoPnJYDR", "trusted": true}, "outputs": [], "source": "X_, X_test, y_, y_test = train_test_split(X_phi, y, test_size=0.25)\nX_train, X_validation, y_train, y_validation = train_test_split(X_, y_, test_size=0.33)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "2MzOmVuhJYDS"}, "source": "### Entrainement du mod\u00e8le sur les donn\u00e9es d'entra\u00eenement\n\nVous pouvez ici changez les hyper-param\u00e8tres pour en voir l'effet sur les diff\u00e9rentes erreurs sur les diff\u00e9rents sous-ensembles de donn\u00e9es. Vous pouvez aussi essayer d'autres [mod\u00e8les lin\u00e9aires](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) (mais juste des regresseurs hein !)."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "fb8QMkP3JYDU", "trusted": true}, "outputs": [], "source": "reg2 = linear_model.ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n      max_iter=100000, normalize=False, positive=False, precompute=False,\n      random_state=None, selection='random', tol=0.0001, warm_start=False)\nreg2.fit(X_train, y_train)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "4qlk23EKJYDW"}, "source": "### \u00c9valuation du mod\u00e8le sur l'ensemble d'entra\u00eenement (training loss, erreur d'entrainement)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "ljlzukE-JYDW", "trusted": true}, "outputs": [], "source": "y_train_pred = reg2.predict(X_train)\ntraining_error = mean_squared_error(y_train, y_train_pred)\nprint(\"L'erreur d'entra\u00eenement du mod\u00e8le appris est : %5.2f\" % training_error)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "ezKAGQMXJYDY"}, "source": "### \u00c9valuation du mod\u00e8le sur l'ensemble de validation (validation loss, erreur de validation)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "_oWIiAYeJYDY", "trusted": true}, "outputs": [], "source": "y_val_pred = reg2.predict(X_validation)\nvalidation_error = mean_squared_error(y_validation, y_val_pred)\nprint(\"L'erreur de validation du mod\u00e8le appris est : %5.2f\" % validation_error)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "HwhFcqzRJYDb"}, "source": "### \u00c9valuation du mod\u00e8le sur l'ensemble de test (test loss, erreur de test)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "CMJ41gKqJYDb", "trusted": true}, "outputs": [], "source": "y_test_pred = reg2.predict(X_test)\ntest_error = mean_squared_error(y_test, y_test_pred)\nprint(\"L'erreur de test du mod\u00e8le appris est : %5.2f\" % test_error)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "u2SbomOxJYDd"}, "source": "### Estimation du vrai risque du mod\u00e8le"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "wj4Ke4elJYDd", "trusted": true}, "outputs": [], "source": "X_risk, y_risk = generate_data(1000000)\nX_risk = feature_space_projection(X_risk, phi)\ny_risk_pred = reg2.predict(X_risk)\ntrue_risk = mean_squared_error(y_risk, y_risk_pred)\nprint(\"L'erreur de g\u00e9n\u00e9ralisation du mod\u00e8le appris est : %5.2f \u00b1 %2.2f\" % (true_risk, confidence_interval(y_risk, y_risk_pred)))"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "Enlxx4ggJYDf"}, "source": "Si on se rappelle que l'ensemble de test et de validation sont statistiquement identiques, on concoit ais\u00e9ment que l'estimation du vrai risque est toujours tr\u00e8s difficile et potentiellement \u00e9loign\u00e9e de l'erreur de validation ou de celle de test. \n\nOn a *optimis\u00e9* plusieurs mod\u00e8les sur l'ensemble d'entrainement, qu'on en a *choisi* les hyperparam\u00e8tres d'apprentissage optimaux sur l'ensemble de validation et qu'on a v\u00e9rifi\u00e9 nos choix **une seule fois** sur l'ensemble de test. "}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "XqubDTHzJYDf"}, "source": "# Exemple 4 : M\u00e9thodologie de validation crois\u00e9e et test"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "KiE2TkO1JYDg"}, "source": "Dans l'exercice pr\u00e9c\u00e9dent, nous avions un seul ensemble de _validation_, i.e. nos hyperparam\u00e8tres d'apprentissage \u00e9taient optimaux pour cet ensembelde de donn\u00e9es la et pouvait mal g\u00e9n\u00e9raliser. La validation crois\u00e9e vient pallier \u00e0 ce probl\u00e8me en optimisant les hyeprparam\u00e8tres sur plusieurs ensembles de validation **diff\u00e9rents** pour plusieurs ensembles d'entrainement **diff\u00e9rents**.\n\nPour ce faire, nous d\u00e9coupons seulement un ensemble de test pour calculer notre erreur empirique.\n\n### splits : train(75%) - test(25%)\n\nUn ensemble d'entrainement sur lequel nous ferons de la validation crois\u00e9e et un ensemble de test pour la v\u00e9rification *finale*."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "dMk4KTn-JYDg", "trusted": true}, "outputs": [], "source": "X_train, X_test, y_train, y_test = train_test_split(X_phi, y, test_size=0.25)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "n2n5aI2CJYDh"}, "source": "## Cross-validation du mod\u00e8le/hyperparam\u00e8tres\n\nVous pouvez ici changez les hyper-param\u00e8tres pour en voir l'effet sur les diff\u00e9rentes erreurs sur les diff\u00e9rents sous-ensembles de donn\u00e9es. Vous pouvez aussi essayer d'autres mod\u00e8les lin\u00e9aires."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "OLv2Iu2PJYDi", "trusted": true}, "outputs": [], "source": "reg2 = linear_model.ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n      max_iter=100000, normalize=False, positive=False, precompute=False,\n      random_state=None, selection='random', tol=0.0001, warm_start=False)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "v7aWcSxcJYDk", "trusted": true}, "outputs": [], "source": "cv_score = cross_val_score(reg2, X_train, y_train, cv=5, scoring='neg_mean_squared_error')"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "fseh4OcjJYDm", "trusted": true}, "outputs": [], "source": "-cv_score"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "s3_1FJkNJYDn", "trusted": true}, "outputs": [], "source": "np.mean(-cv_score)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "XM3mKUSvJYDo"}, "source": "## Recherche en grille des hyperparam\u00e8tres\n\nPlutot que de rechercher *au jug\u00e9* les hyperparm\u00e8tres, il est plus int\u00e9ressant, quand c'est possible, de *tous* les essayer."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "R89w6A68JYDp", "trusted": true}, "outputs": [], "source": "reg2 = linear_model.ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n      max_iter=1000000, normalize=False, positive=False, precompute=False,\n      random_state=None, selection='random', tol=0.0001, warm_start=False)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "KQo1bQ-jJYDr"}, "source": "Nous d\u00e9finissons donc ici, tous les couples d'hyperparm\u00e8tres \u00e0 utiliser."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "CdR0df8wJYDr", "trusted": true}, "outputs": [], "source": "hyperparameters = {'l1_ratio':[0.0, 0.25, 0.5, 0.75, 1.0], 'alpha':[0.01, 0.1, 1, 10, 100]}"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "4Jtr62p3JYDt", "trusted": true}, "outputs": [], "source": "clf = GridSearchCV(reg2, hyperparameters, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "3jXHEymcJYDu", "trusted": true}, "outputs": [], "source": "clf.fit(X_train, y_train)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "iosfx-xUJYDv", "trusted": true}, "outputs": [], "source": "clf.best_params_"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "pA2k3ZN3JYDx"}, "source": "Curieusement, quand on voit les r\u00e9sultats des *meilleurs* hyperparm\u00e8tres, l'envie de raffiner votre grille de recherche dans la bonne r\u00e9gion se fait probalement sentir ..."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "LCW-hXBfJYDx"}, "source": "## Entra\u00eenement et \u00e9valuation sur le test\n\nUne fois les meilleurs hyperparam\u00e8tres choisis, on les utilise pour aprendre sur l'ensemble des donn\u00e9es d'entrainement et v\u00e9rifier une derni\u00e8re fois notre apprentissage sur l'ensemble de test."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "ZKPojFJ9JYDx", "trusted": true}, "outputs": [], "source": "best_alpha = clf.best_params_['alpha']\nbest_l1ratio = clf.best_params_['l1_ratio']"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "c6MLql-FJYDy", "trusted": true}, "outputs": [], "source": "reg2 = linear_model.ElasticNet(alpha=best_alpha, copy_X=True, fit_intercept=True, l1_ratio=best_l1ratio,\n      max_iter=100000, normalize=False, positive=False, precompute=False,\n      random_state=None, selection='random', tol=0.0001, warm_start=False)\nreg2.fit(X_train, y_train)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "SjFnaZWeJYD1", "trusted": true}, "outputs": [], "source": "y_train_pred = reg2.predict(X_train)\ntraining_error = mean_squared_error(y_train, y_train_pred)\nprint(\"L'erreur d'entra\u00eenement du mod\u00e8le appris est : %5.2f\" % training_error)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "ngmlw5NPJYD2", "trusted": true}, "outputs": [], "source": "y_test_pred = reg2.predict(X_test)\ntest_error = mean_squared_error(y_test, y_test_pred)\nprint(\"L'erreur de test du mod\u00e8le appris est : %5.2f \u00b1 %2.2f\" % (test_error, confidence_interval(y_test, y_test_pred)))"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "editable": false, "id": "-GPXO3W4JYD3"}, "source": "On voit ici que l'intervalle de confiance et autour de $\\pm 30\\%$, impliquant que le *vrai* risque est probablement assez \u00e9loign\u00e9 du risque obtenu.\n\nAffichage du mod\u00e8le appris :"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "id": "XCbJkk0fJYD6", "trusted": true}, "outputs": [], "source": "fig, ax = plt.subplots(figsize=(8,6))\nax.plot(X, y, 'o')\nax.plot(np.linspace(-10,10,100), np.polyval(listeParamPoly,np.linspace(-10,10,100)), color='black', linewidth=3)\nax.set_title('Polyn\u00f4me')\nax.set_ylabel('y')\nax.set_xlabel('x')\n\nlinspace_x = np.linspace(-10, 10, num=100000)\nlinspace_x = np.expand_dims(linspace_x, axis=1)\n\nlinspace_X_augmented = feature_space_projection(linspace_x, phi)\n\ny_pred = reg2.predict(linspace_X_augmented)\n#ax.plot(linspace_x, y_pred, color='red', linewidth=3)\n\nplt.show()"}], "metadata": {"PAX": {"revision": 798, "userLang": "fr"}, "celltoolbar": "", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.1"}, "revision": 798}, "nbformat": 4, "nbformat_minor": 2}